{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14583445,"sourceType":"datasetVersion","datasetId":9315761},{"sourceId":14590869,"sourceType":"datasetVersion","datasetId":9320053}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:11.392277Z","iopub.execute_input":"2026-01-23T08:37:11.392568Z","iopub.status.idle":"2026-01-23T08:37:12.920258Z","shell.execute_reply.started":"2026-01-23T08:37:11.392527Z","shell.execute_reply":"2026-01-23T08:37:12.918957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c l√†m vi·ªác\nworking_dir = \"/kaggle/working\"\n\n# X√≥a t·∫•t c·∫£ file v√† th∆∞ m·ª•c con trong /kaggle/working\nfor filename in os.listdir(working_dir):\n    file_path = os.path.join(working_dir, filename)\n    try:\n        if os.path.isfile(file_path) or os.path.islink(file_path):\n            os.unlink(file_path)  # X√≥a file\n        elif os.path.isdir(file_path):\n            shutil.rmtree(file_path)  # X√≥a th∆∞ m·ª•c\n    except Exception as e:\n        print(f\"Kh√¥ng th·ªÉ x√≥a {file_path}: {e}\")\n\nprint(\"‚úÖ ƒê√£ x√≥a to√†n b·ªô file c≈© trong /kaggle/working\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:12.922361Z","iopub.execute_input":"2026-01-23T08:37:12.923006Z","iopub.status.idle":"2026-01-23T08:37:12.933460Z","shell.execute_reply.started":"2026-01-23T08:37:12.922958Z","shell.execute_reply":"2026-01-23T08:37:12.931930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nComplete WSN Clustering Simulation with 100 Nodes\n- 3 Distribution Scenarios: Uniform Random, Corner-Biased, Clustered\n- 3 Algorithms: AC-ACO, Basic ACO, GA\n- Full logging every 250 rounds\n- Input/Output file export\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass, asdict\nfrom typing import List, Tuple, Dict\nimport json\nimport csv\nfrom datetime import datetime\nimport os\n\n# Set random seed\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:12.934732Z","iopub.execute_input":"2026-01-23T08:37:12.935062Z","iopub.status.idle":"2026-01-23T08:37:12.953974Z","shell.execute_reply.started":"2026-01-23T08:37:12.935004Z","shell.execute_reply":"2026-01-23T08:37:12.952917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:12.955163Z","iopub.execute_input":"2026-01-23T08:37:12.955572Z","iopub.status.idle":"2026-01-23T08:37:18.167016Z","shell.execute_reply.started":"2026-01-23T08:37:12.955527Z","shell.execute_reply":"2026-01-23T08:37:18.165656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# NETWORK PARAMETERS (for 200-node WSN)\n# ============================================================================\n\nfrom dataclasses import dataclass, asdict\nfrom typing import Tuple\nimport numpy as np\n\n@dataclass\nclass NetworkParams:\n    \"\"\"Network parameters for WSN with 200 nodes\"\"\"\n    area_size: Tuple[int, int] = (250, 250)\n    n_nodes: int = 500                      # ‚úÖ changed from 200 ‚Üí 200\n    BS_location: Tuple[float, float] = (125.0, 125.0)\n    R: float = 20.0\n    packet_length: int = 2000\n    ctrl_packet_length: int = 100\n    E0: float = 0.6                         # ‚úÖ increased from 0.5 ‚Üí 0.6\n    Eelec: float = 50e-9\n    Efs: float = 10e-12\n    Emp: float = 0.0013e-12\n    EDA: float = 5e-9\n    p_CH: float = 0.1\n    Tmax: int = 3500                        # ‚úÖ simulation rounds = 2500\n\n# ============================================================================\n# NETWORK PARAMETERS (for 200-node WSN)\n# ============================================================================\n\nfrom dataclasses import dataclass, asdict\nfrom typing import Tuple\nimport numpy as np\n\n@dataclass\nclass NetworkParams:\n    \"\"\"Network parameters for WSN with 200 nodes\"\"\"\n    area_size: Tuple[int, int] = (250, 250)\n    n_nodes: int = 500                      # ‚úÖ changed from 200 ‚Üí 200\n    BS_location: Tuple[float, float] = (125.0, 125.0)\n    R: float = 20.0\n    packet_length: int = 2000\n    ctrl_packet_length: int = 100\n    E0: float = 0.6                         # ‚úÖ increased from 0.5 ‚Üí 0.6\n    Eelec: float = 50e-9\n    Efs: float = 10e-12\n    Emp: float = 0.0013e-12\n    EDA: float = 5e-9\n    p_CH: float = 0.1\n    Tmax: int = 3500                        # ‚úÖ simulation rounds = 2500\n\n    def __post_init__(self):\n        self.d0 = np.sqrt(self.Efs / self.Emp)\n\n    # def to_dict(self):\n    #     return asdict(self)\n    def to_dict(self):\n        return {\n            'area_size': self.area_size,\n            'n_nodes': self.n_nodes,\n            'BS_location': self.BS_location,\n            'R': self.R,\n            'packet_length': self.packet_length,\n            'ctrl_packet_length': self.ctrl_packet_length,\n            'E0': self.E0,\n            'Eelec': self.Eelec,\n            'Efs': self.Efs,\n            'Emp': self.Emp,\n            'EDA': self.EDA,\n            'p_CH': self.p_CH,\n            'Tmax': self.Tmax,\n            'd0': self.d0\n        }\n\n    # def to_dict(self):\n    #     return asdict(self)\n    def to_dict(self):\n        return {\n            'area_size': self.area_size,\n            'n_nodes': self.n_nodes,\n            'BS_location': self.BS_location,\n            'R': self.R,\n            'packet_length': self.packet_length,\n            'ctrl_packet_length': self.ctrl_packet_length,\n            'E0': self.E0,\n            'Eelec': self.Eelec,\n            'Efs': self.Efs,\n            'Emp': self.Emp,\n            'EDA': self.EDA,\n            'p_CH': self.p_CH,\n            'Tmax': self.Tmax,\n            'd0': self.d0\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.170031Z","iopub.execute_input":"2026-01-23T08:37:18.171536Z","iopub.status.idle":"2026-01-23T08:37:18.187482Z","shell.execute_reply.started":"2026-01-23T08:37:18.171488Z","shell.execute_reply":"2026-01-23T08:37:18.186457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# NODE AND NETWORK CLASSES\n# ============================================================================\n\nclass Node:\n    def __init__(self, idx: int, x: float, y: float, energy: float):\n        self.idx = idx\n        self.x = x\n        self.y = y\n        self.energy = energy\n        self.is_alive = True\n        self.is_CH = False\n\n    def distance_to(self, other) -> float:\n        if isinstance(other, Node):\n            return np.sqrt((self.x - other.x)**2 + (self.y - other.y)**2)\n        else:\n            return np.sqrt((self.x - other[0])**2 + (self.y - other[1])**2)\n\n\n# class WSN:\n#     def __init__(self, params: NetworkParams, node_positions: List[Tuple[float, float]] = None):\n#         self.params = params\n#         self.nodes = self._initialize_nodes(node_positions)\n\nclass WSN:\n    def __init__(self, params, node_positions):\n        self.params = params\n        self.n = params.n_nodes\n\n        self.nodes = self._initialize_nodes(node_positions)\n    \n        pos = torch.tensor(node_positions, dtype=torch.float32, device=device)\n        self.x = pos[:, 0]\n        self.y = pos[:, 1]\n        \n        self.energy = torch.full((self.n,), params.E0, device=device)\n        self.alive = torch.ones(self.n, dtype=torch.bool, device=device)\n    \n      \n\n    def _initialize_nodes(self, positions: List[Tuple[float, float]] = None) -> List[Node]:\n        nodes = []\n        if positions:\n            for i, (x, y) in enumerate(positions):\n                node = Node(i, x, y, self.params.E0)\n                nodes.append(node)\n        else:\n            for i in range(self.params.n_nodes):\n                x = np.random.uniform(0, self.params.area_size[0])\n                y = np.random.uniform(0, self.params.area_size[1])\n                node = Node(i, x, y, self.params.E0)\n                nodes.append(node)\n        return nodes\n\n    def get_alive_nodes(self):\n        return [self.nodes[i] for i in range(self.n) if self.alive[i]]\n\n\n    # def calculate_energy_consumption(self, CH_indices: List[int],\n    #                                  clusters: Dict[int, List[int]]) -> np.ndarray:\n    #     E_consumed = np.zeros(self.params.n_nodes)\n\n    #     for ch_idx in CH_indices:\n    #         ch_node = self.nodes[ch_idx]\n    #         if not ch_node.is_alive:\n    #             continue\n\n    #         member_indices = clusters.get(ch_idx, [])\n    #         E_agg = self.params.EDA * self.params.packet_length * len(member_indices)\n    #         E_consumed[ch_idx] += E_agg\n\n    #         for member_idx in member_indices:\n    #             member = self.nodes[member_idx]\n    #             if not member.is_alive:\n    #                 continue\n\n    #             dist = member.distance_to(ch_node)\n\n    #             if dist < self.params.d0:\n    #                 E_tx_ctrl = (self.params.Eelec * self.params.ctrl_packet_length +\n    #                              self.params.Efs * self.params.ctrl_packet_length * dist**2)\n    #             else:\n    #                 E_tx_ctrl = (self.params.Eelec * self.params.ctrl_packet_length +\n    #                              self.params.Emp * self.params.ctrl_packet_length * dist**4)\n    #             E_consumed[member_idx] += E_tx_ctrl\n\n    #             E_rx_ctrl = self.params.Eelec * self.params.ctrl_packet_length\n    #             E_consumed[ch_idx] += E_rx_ctrl\n\n    #             if dist < self.params.d0:\n    #                 E_tx = (self.params.Eelec * self.params.packet_length +\n    #                         self.params.Efs * self.params.packet_length * dist**2)\n    #             else:\n    #                 E_tx = (self.params.Eelec * self.params.packet_length +\n    #                         self.params.Emp * self.params.packet_length * dist**4)\n    #             E_consumed[member_idx] += E_tx\n\n    #             E_rx = self.params.Eelec * self.params.packet_length\n    #             E_consumed[ch_idx] += E_rx\n\n    #         dist_BS = ch_node.distance_to(self.params.BS_location)\n    #         total_data_length = self.params.packet_length\n    #         if dist_BS < self.params.d0:\n    #             E_tx_BS = (self.params.Eelec * total_data_length +\n    #                        self.params.Efs * total_data_length * dist_BS**2)\n    #         else:\n    #             E_tx_BS = (self.params.Eelec * total_data_length +\n    #                        self.params.Emp * total_data_length * dist_BS**4)\n    #         E_consumed[ch_idx] += E_tx_BS\n\n    #     return E_consumed\n\n\n    def calculate_energy_consumption(self, CH_indices, clusters):\n        p = self.params\n        device = self.energy.device\n    \n        n = self.energy.shape[0]\n        E = torch.zeros(n, device=device)\n    \n        CH = torch.tensor(CH_indices, device=device, dtype=torch.long)\n    \n        for ch in CH:\n            ch = int(ch.item())\n            if not self.alive[ch]:\n                continue\n    \n            members = clusters.get(ch, [])\n            if not members:\n                continue\n    \n            members = torch.tensor(members, device=device, dtype=torch.long)\n    \n            alive_mask = self.alive[members]\n            members = members[alive_mask]\n            if len(members) == 0:\n                continue\n    \n            dx = self.x[members] - self.x[ch]\n            dy = self.y[members] - self.y[ch]\n            dist = torch.sqrt(dx**2 + dy**2)\n    \n            # Control TX\n            E[members] += torch.where(\n                dist < p.d0,\n                p.Eelec * p.ctrl_packet_length + p.Efs * p.ctrl_packet_length * dist**2,\n                p.Eelec * p.ctrl_packet_length + p.Emp * p.ctrl_packet_length * dist**4\n            )\n    \n            # CH RX control\n            E[ch] += p.Eelec * p.ctrl_packet_length * len(members)\n    \n            # Data TX\n            E[members] += torch.where(\n                dist < p.d0,\n                p.Eelec * p.packet_length + p.Efs * p.packet_length * dist**2,\n                p.Eelec * p.packet_length + p.Emp * p.packet_length * dist**4\n            )\n    \n            # CH RX data\n            E[ch] += p.Eelec * p.packet_length * len(members)\n    \n            # CH ‚Üí BS\n            dx_bs = self.x[ch] - p.BS_location[0]\n            dy_bs = self.y[ch] - p.BS_location[1]\n            dist_bs = torch.sqrt(dx_bs**2 + dy_bs**2)\n    \n            E[ch] += torch.where(\n                dist_bs < p.d0,\n                p.Eelec * p.packet_length + p.Efs * p.packet_length * dist_bs**2,\n                p.Eelec * p.packet_length + p.Emp * p.packet_length * dist_bs**4\n            )\n    \n        return E\n\n\n    # def update_energy(self, E_consumed: np.ndarray):\n    #     for i, node in enumerate(self.nodes):\n    #         node.energy = max(0, node.energy - E_consumed[i])\n    #         if node.energy == 0:\n    #             node.is_alive = False\n\n    def update_energy(self, E_consumed):\n        self.energy = torch.clamp(self.energy - E_consumed, min=0.0)\n        self.alive = self.energy > 0\n    \n        # üî• sync ng∆∞·ª£c ƒë·ªÉ c√°c thu·∫≠t to√°n c≈© kh√¥ng ch·∫øt\n        for i, node in enumerate(self.nodes):\n            node.energy = float(self.energy[i].item())\n            node.is_alive = bool(self.alive[i].item())\n\n\n    def copy(self):\n        new_wsn = WSN.__new__(WSN)\n    \n        new_wsn.params = self.params\n        new_wsn.n = self.n   # üî• FIX L·ªñI ·ªû ƒê√ÇY\n    \n        # Copy Node objects (CPU side)\n        new_wsn.nodes = [Node(n.idx, n.x, n.y, n.energy) for n in self.nodes]\n        for i, node in enumerate(new_wsn.nodes):\n            node.is_alive = self.nodes[i].is_alive\n            node.is_CH = self.nodes[i].is_CH\n    \n        # üî• Copy GPU tensors\n        new_wsn.x = self.x.clone()\n        new_wsn.y = self.y.clone()\n        new_wsn.energy = self.energy.clone()\n        new_wsn.alive = self.alive.clone()\n    \n        return new_wsn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.188764Z","iopub.execute_input":"2026-01-23T08:37:18.189132Z","iopub.status.idle":"2026-01-23T08:37:18.223330Z","shell.execute_reply.started":"2026-01-23T08:37:18.189094Z","shell.execute_reply":"2026-01-23T08:37:18.222122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @dataclass\n# class GAParams:\n#     \"\"\"Genetic Algorithm parameters (inefficient / weak version)\"\"\"\n#     population_size: int = 5          # üîª Gi·∫£m t·ª´ 20 ‚Üí 5\n#     crossover_rate: float = 0.3       # üîª Gi·∫£m kh·∫£ nƒÉng lai gh√©p\n#     mutation_rate: float = 0.01       # üîª Gi·∫£m kh·∫£ nƒÉng ƒë·ªôt bi·∫øn\n#     elite_size: int = 1               # üîª Ch·ªâ gi·ªØ 1 c√° th·ªÉ t·ªët nh·∫•t\n#     tournament_size: int = 8          # üî∫ TƒÉng cao ‚Üí ch·ªçn l·ªçc s·ªõm, m·∫•t ƒëa d·∫°ng\n\n#     def to_dict(self):\n#         return asdict(self)\n\n@dataclass\nclass GAParams:\n    \"\"\"Genetic Algorithm parameters (balanced-fast version for 300-node WSN)\"\"\"\n    # population_size = 80\n    # crossover_rate = 0.4\n    # mutation_rate = 0.025\n    # elite_size = 3\n    # tournament_size = 6\n\n    # population_size = 50\n    # crossover_rate = 0.35\n    # mutation_rate = 0.025\n    # elite_size = 3\n    # tournament_size = 6\n\n\n    population_size = 10\n    crossover_rate = 0.3\n    mutation_rate = 0.02\n    elite_size = 2\n    tournament_size = 3\n\n    # population_size: int = 5          # üîª Gi·∫£m t·ª´ 20 ‚Üí 5\n    # crossover_rate: float = 0.3       # üîª Gi·∫£m kh·∫£ nƒÉng lai gh√©p\n    # mutation_rate: float = 0.01       # üîª Gi·∫£m kh·∫£ nƒÉng ƒë·ªôt bi·∫øn\n    # elite_size: int = 1               # üîª Ch·ªâ gi·ªØ 1 c√° th·ªÉ t·ªët nh·∫•t\n    # tournament_size: int = 3  \n    \n\n    def to_dict(self):\n        return asdict(self)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.224724Z","iopub.execute_input":"2026-01-23T08:37:18.225232Z","iopub.status.idle":"2026-01-23T08:37:18.250021Z","shell.execute_reply.started":"2026-01-23T08:37:18.225170Z","shell.execute_reply":"2026-01-23T08:37:18.248956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# ACO PARAMETERS (for ACO / AC-ACO)\n# ============================================================================\n\n@dataclass\nclass ACOParams:\n    \"\"\"ACO algorithm parameters\"\"\"\n    m: int =10\n    alpha: float = 1.0\n    beta_init: float = 3.0\n    gamma: float = 0.1\n    rho_max: float = 0.9\n    rho_min: float = 0.1\n    beta_min: float = 1.0\n    beta_max: float = 5.0\n    alpha_chaos_min: float = 0.05\n    alpha_chaos_max: float = 0.3\n    tau0: float = 1.0\n    r: float = 3.61\n    Q: float = 100.0\n    k: float = 5.0\n    r_max = 4.0\n    r_min = 3.6\n\n\n    def to_dict(self):\n        return asdict(self)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.251561Z","iopub.execute_input":"2026-01-23T08:37:18.252014Z","iopub.status.idle":"2026-01-23T08:37:18.281710Z","shell.execute_reply.started":"2026-01-23T08:37:18.251975Z","shell.execute_reply":"2026-01-23T08:37:18.280532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ClusterCalculator:\n    \"\"\"C√°c ph∆∞∆°ng ph√°p t√≠nh s·ªë c·ª•m (c·∫£i ti·∫øn: ƒë·ªông theo m·∫°ng)\"\"\"\n    \n    @staticmethod\n    def k_old(params: NetworkParams, alive_nodes: List[Node]) -> int:\n        \"\"\"\n        C√°ch c≈© (ƒë·ªông nh·∫π): k = p_CH * s·ªë node c√≤n s·ªëng\n        \"\"\"\n        if not alive_nodes:\n            return 0\n        return max(1, int(round(params.p_CH * len(alive_nodes))))\n    \n    # @staticmethod\n\n    \n    # def k_opt_leach(params: NetworkParams, alive_nodes: List[Node]) -> int:\n    #     N = len(alive_nodes)\n    #     if N == 0:\n    #         return 0\n    \n    #     # --- k√≠ch th∆∞·ªõc c·∫°nh v√πng m·∫°ng (LEACH chu·∫©n d√πng c·∫°nh, kh√¥ng d√πng di·ªán t√≠ch)\n    #     M_side = np.sqrt(params.area_size[0] * params.area_size[1])\n    \n    #     # --- kho·∫£ng c√°ch trung b√¨nh ƒë·∫øn BS (thay ƒë·ªïi theo round)\n    #     d_to_BS = np.mean([node.distance_to(params.BS_location) for node in alive_nodes])\n    \n    #     # --- nƒÉng l∆∞·ª£ng trung b√¨nh v√† t·ªâ l·ªá so v·ªõi ban ƒë·∫ßu (thay ƒë·ªïi theo round)\n    #     avg_energy = np.mean([node.energy for node in alive_nodes])\n    #     energy_ratio = avg_energy / params.E0 if params.E0 > 0 else 0.0\n    #     energy_ratio = float(np.clip(energy_ratio, 0.0, 1.0))\n    \n    #     # --- k_opt base theo LEACH chu·∫©n\n    #     k_base = np.sqrt(N / (2 * np.pi)) * np.sqrt(params.Efs / params.Emp) * (M_side / (d_to_BS + 1e-9))\n    \n    #     # --- SCALE theo nƒÉng l∆∞·ª£ng: bounded linear scaling (an to√†n)\n    #     # B·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh min_scale/max_scale theo m·ª•c ti√™u ti·∫øt ki·ªám nƒÉng l∆∞·ª£ng\n    #     min_scale = 0.5   # khi m·∫°ng y·∫øu, t·ªëi thi·ªÉu c√≤n 50% s·ªë c·ª•m chu·∫©n\n    #     max_scale = 1.2   # khi m·∫°ng kh·ªèe, t·ªëi ƒëa 120% s·ªë c·ª•m chu·∫©n\n    #     # map energy_ratio [0,1] -> scale [min_scale, max_scale]\n    #     scale = min_scale + (max_scale - min_scale) * energy_ratio\n    \n    #     k_opt = k_base * scale\n    \n    #     # optional: th√™m h·ªá s·ªë ƒëi·ªÅu ch·ªânh to√†n c·ª•c nh·ªè (n·∫øu b·∫°n ƒë√£ d√πng scale ph√π h·ª£p, c√≥ th·ªÉ kh√¥ng c·∫ßn)\n    #     k_opt *= 0.025  # n·∫øu b·∫°n v·∫´n mu·ªën gi·∫£m m·∫°nh t·ªïng th·ªÉ\n    \n    #     return max(1, int(round(k_opt)))\n\n\n    @staticmethod\n    def k_opt_leach(params: NetworkParams, alive_nodes: List[Node]) -> int:\n        N = len(alive_nodes)\n        if N == 0:\n            return 0\n    \n        alive_ratio = N / params.n_nodes\n    \n        # --- Pha b·∫£o v·ªá FND\n        if alive_ratio > 0.9:\n            return max(1, int(round(params.p_CH * N)))\n    \n        M_side = np.sqrt(params.area_size[0] * params.area_size[1])\n        d_to_BS = np.mean([node.distance_to(params.BS_location) for node in alive_nodes])\n    \n        k_base = np.sqrt(N / (2 * np.pi)) \\\n                 * np.sqrt(params.Efs / params.Emp) \\\n                 * (M_side / (d_to_BS + 1e-9))\n    \n        # --- scale theo m·∫≠t ƒë·ªô s·ªëng (t·ªët cho FND)\n        min_scale = 0.4\n        max_scale = 1.1\n        scale = min_scale + (max_scale - min_scale) * (1 - alive_ratio)\n    \n        k_opt = k_base * scale * 0.025\n    \n        # --- cap CH giai ƒëo·∫°n ƒë·∫ßu\n        k_opt = min(k_opt, int(0.04 * N))\n    \n        return max(1, int(round(k_opt)))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.283247Z","iopub.execute_input":"2026-01-23T08:37:18.284198Z","iopub.status.idle":"2026-01-23T08:37:18.315408Z","shell.execute_reply.started":"2026-01-23T08:37:18.284166Z","shell.execute_reply":"2026-01-23T08:37:18.314389Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# NODE DISTRIBUTION SCENARIOS\n# ============================================================================\n\nclass NodeDistribution:\n    @staticmethod\n    def uniform_random(n_nodes: int, area_size: Tuple[int, int]) -> List[Tuple[float, float]]:\n        positions = []\n        for _ in range(n_nodes):\n            x = np.random.uniform(0, area_size[0])\n            y = np.random.uniform(0, area_size[1])\n            positions.append((x, y))\n        return positions\n\n    @staticmethod\n    def corner_biased(n_nodes: int, area_size: Tuple[int, int],\n                     bias_ratio: float = 0.85) -> List[Tuple[float, float]]:\n        positions = []\n        for _ in range(n_nodes):\n            if np.random.rand() < bias_ratio:\n                x = np.random.uniform(0, area_size[0] * 0.4)\n                y = np.random.uniform(0, area_size[1] * 0.4)\n            else:\n                x = np.random.uniform(0, area_size[0])\n                y = np.random.uniform(0, area_size[1])\n            positions.append((x, y))\n        return positions\n\n    @staticmethod\n    def clustered(n_nodes: int, area_size: Tuple[int, int],\n                 n_clusters: int = 4) -> List[Tuple[float, float]]:\n        positions = []\n        centers = [(50, 50), (200, 50), (50, 200), (200, 200)]\n        nodes_per_cluster = n_nodes // len(centers)\n        remaining = n_nodes % len(centers)\n\n        for i, (cx, cy) in enumerate(centers):\n            n_in_cluster = nodes_per_cluster + (1 if i < remaining else 0)\n            std_dev = min(area_size) * 0.08\n\n            for _ in range(n_in_cluster):\n                x = np.clip(np.random.normal(cx, std_dev), 0, area_size[0])\n                y = np.clip(np.random.normal(cy, std_dev), 0, area_size[1])\n                positions.append((x, y))\n\n        return positions\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.316922Z","iopub.execute_input":"2026-01-23T08:37:18.317328Z","iopub.status.idle":"2026-01-23T08:37:18.339001Z","shell.execute_reply.started":"2026-01-23T08:37:18.317288Z","shell.execute_reply":"2026-01-23T08:37:18.337848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# BASE ALGORITHM CLASS\n# ============================================================================\n\nclass BaseAlgorithm:\n    def __init__(self, wsn: WSN, net_params: NetworkParams,\n                 scenario_name=\"default_scenario\",\n                 algorithm_name=\"BaseAlgorithm\",\n                 fitness_type=\"old\",\n                 file_manager=None):\n\n        self.wsn = wsn\n        self.net_params = net_params\n        self.scenario_name = scenario_name\n        self.algorithm_name = algorithm_name\n        self.fitness_type = fitness_type\n        self.file_manager = file_manager\n\n        self.stats = {\n            'total_energy': [], 'avg_energy': [], 'alive_nodes': [],\n            'dead_rounds': [], 'cumulative_energy': [0.0],\n            'best_fitness': [], 'n_clusters': [],\n            'clusters_per_round': []      # NEW: L∆∞u l·ªãch s·ª≠ ph√¢n c·ª•m\n        }\n\n\n    # def form_clusters(self, CH_indices: List[int]) -> Dict[int, List[int]]:\n    #     clusters = {ch_idx: [] for ch_idx in CH_indices}\n    #     if not CH_indices:\n    #         return clusters\n    #     for node in self.wsn.get_alive_nodes():\n    #         if node.idx in CH_indices:\n    #             continue\n    #         min_dist, nearest_CH = float('inf'), -1\n    #         for ch_idx in CH_indices:\n    #             dist = node.distance_to(self.wsn.nodes[ch_idx])\n    #             if dist < min_dist:\n    #                 min_dist, nearest_CH = dist, ch_idx\n    #         if nearest_CH != -1:\n    #             clusters[nearest_CH].append(node.idx)\n    #     return clusters\n\n\n    def form_clusters(self, CH_indices):\n        device = self.device\n    \n        CH = torch.tensor(CH_indices, device=device)\n    \n        # üî• FIX: x√°c ƒë·ªãnh node c√≤n s·ªëng d·ª±a tr√™n energy\n        energies = torch.tensor(\n            [n.energy for n in self.wsn.nodes],\n            device=device\n        )\n    \n        alive_nodes = torch.where(energies > 0)[0]\n    \n        clusters = {int(ch): [] for ch in CH_indices}\n    \n        # T·ªça ƒë·ªô\n        x = torch.tensor([n.x for n in self.wsn.nodes], device=device)\n        y = torch.tensor([n.y for n in self.wsn.nodes], device=device)\n    \n        CH_x = x[CH]\n        CH_y = y[CH]\n    \n        # G√°n node ‚Üí CH g·∫ßn nh·∫•t\n        for node_idx in alive_nodes.tolist():\n            if node_idx in CH_indices:\n                continue\n    \n            dx = CH_x - x[node_idx]\n            dy = CH_y - y[node_idx]\n            dists = torch.sqrt(dx**2 + dy**2)\n    \n            nearest_ch = CH_indices[int(torch.argmin(dists))]\n            clusters[int(nearest_ch)].append(int(node_idx))\n    \n        return clusters\n\n    # def form_clusters(self, CH_indices, alpha=0.4, beta=0.6):\n    #     device = self.device\n    \n    #     CH = torch.tensor(CH_indices, device=device)\n    \n    #     # Energy c√°c node\n    #     energies = torch.tensor(\n    #         [n.energy for n in self.wsn.nodes],\n    #         device=device\n    #     )\n    \n        # # Node c√≤n s·ªëng\n        # alive_nodes = torch.where(energies > 0)[0]\n    \n        # clusters = {int(ch): [] for ch in CH_indices}\n    \n        # # T·ªça ƒë·ªô\n        # x = torch.tensor([n.x for n in self.wsn.nodes], device=device)\n        # y = torch.tensor([n.y for n in self.wsn.nodes], device=device)\n    \n        # CH_x = x[CH]\n        # CH_y = y[CH]\n    \n        # # NƒÉng l∆∞·ª£ng c√≤n l·∫°i c·ªßa CH\n        # CH_energy = energies[CH]  # Erest_xj\n    \n        # for node_idx in alive_nodes.tolist():\n        #     if node_idx in CH_indices:\n        #         continue\n    \n        #     # Kho·∫£ng c√°ch dij\n        #     dx = CH_x - x[node_idx]\n        #     dy = CH_y - y[node_idx]\n        #     dists = torch.sqrt(dx ** 2 + dy ** 2)\n    \n        #     # H√†m clustering Ri = Œ±*dij ‚àí Œ≤*Erest\n        #     Ri = alpha * dists - beta * CH_energy\n    \n        #     best_ch_idx = torch.argmin(Ri)\n        #     best_ch = CH_indices[int(best_ch_idx)]\n\n        #     clusters[int(best_ch)].append(int(node_idx))\n    \n        # return clusters\n\n\n\n\n\n    def evaluate_CH_fitness_old(self, alive_nodes: List[Node], CH_indices: List[int]) -> float:\n        if not CH_indices:\n            return 0.0\n        E_CH = np.mean([alive_nodes[i].energy for i in CH_indices])\n        non_CH_indices = [i for i in range(len(alive_nodes)) if i not in CH_indices]\n        E_nonCH = np.mean([alive_nodes[i].energy for i in non_CH_indices]) if non_CH_indices else 0.0\n        fitness = E_CH + E_nonCH\n        return fitness\n    \n    # Fitness m·ªõi\n    # def evaluate_CH_fitness_new(self, alive_nodes: List[Node], CH_indices: List[int]) -> float:\n\n    #     if not CH_indices:\n    #         return 0.0\n    #     avg_energy = np.mean([alive_nodes[i].energy for i in CH_indices])\n    #     dist_sum = sum(alive_nodes[CH_indices[i]].distance_to(alive_nodes[CH_indices[j]])\n    #                    for i in range(len(CH_indices)) for j in range(i + 1, len(CH_indices)))\n    #     dist_BS = sum(alive_nodes[i].distance_to(self.net_params.BS_location) for i in CH_indices)\n    #     fitness = avg_energy + 0.1 * dist_sum - 0.01 * dist_BS\n\n    #         # √âp fitness lu√¥n d∆∞∆°ng\n    #     # return max(fitness, 1e-6)  # tr√°nh = 0, tr√°nh √¢m\n            \n    #     return fitness\n\n    def evaluate_CH_fitness_new(\n        self,\n        alive_nodes: List[Node],\n        CH_indices: List[int]\n    ) -> float:\n    \n        if not CH_indices:\n            return 0.0\n    \n        # ==============================\n        # 1Ô∏è‚É£ C√°c th√†nh ph·∫ßn c∆° b·∫£n\n        # ==============================\n        energies = [alive_nodes[i].energy for i in CH_indices]\n        avg_energy = np.mean(energies)\n        min_energy = np.min(energies)\n    \n        dist_sum = sum(\n            alive_nodes[CH_indices[i]].distance_to(alive_nodes[CH_indices[j]])\n            for i in range(len(CH_indices))\n            for j in range(i + 1, len(CH_indices))\n        )\n    \n        dist_BS = sum(\n            alive_nodes[i].distance_to(self.net_params.BS_location)\n            for i in CH_indices\n        )\n    \n        # ==============================\n        # 2Ô∏è‚É£ Environment factor (FIXED)\n        # ==============================\n        # N·∫øu node ch∆∞a c√≥ env ‚Üí d√πng h·∫±ng s·ªë\n        env_value = 0.8   # ‚≠ê gi√° tr·ªã th√¥ng d·ª•ng, ·ªïn ƒë·ªãnh\n        env_factor = env_value\n    \n        # ==============================\n        # 3Ô∏è‚É£ Tr·ªçng s·ªë (weights)\n        # ==============================\n        w_energy = 0.5     # ∆∞u ti√™n nƒÉng l∆∞·ª£ng\n        w_min    = 0.3     # ‚≠ê b·∫£o v·ªá node y·∫øu (c·∫£i thi·ªán FND)\n        w_dist   = 0.1     # ph√¢n t√°n CH\n        w_bs     = 0.1     # g·∫ßn BS\n    \n        # ==============================\n        # 4Ô∏è‚É£ Fitness t·ªïng\n        # ==============================\n        fitness = (\n            w_energy * avg_energy +\n            w_min * min_energy +\n            w_dist * dist_sum -\n            w_bs * dist_BS\n        )\n    \n        # ==============================\n        # 5Ô∏è‚É£ √Åp d·ª•ng m√¥i tr∆∞·ªùng\n        # ==============================\n        fitness *= env_factor\n    \n        # ==============================\n        # 6Ô∏è‚É£ √âp d∆∞∆°ng (an to√†n GA)\n        # ==============================\n        return max(fitness, 1e-6)\n\n\n        \n    # Th√™m v√†o class BaseAlgorithm:\n    def run_round_with_visualization(self, round_num: int, scenario_name: str, \n                                     algorithm_name: str, output_dir: str,\n                                     visualize_interval: int = 500):\n        \"\"\"\n        Run round v·ªõi visualization m·ªói N rounds\n        \n        Args:\n            round_num: S·ªë round hi·ªán t·∫°i\n            scenario_name: T√™n k·ªãch b·∫£n\n            algorithm_name: T√™n thu·∫≠t to√°n\n            output_dir: Th∆∞ m·ª•c l∆∞u ·∫£nh\n            visualize_interval: V·∫Ω bi·ªÉu ƒë·ªì m·ªói N rounds\n        \"\"\"\n        if not self.wsn.get_alive_nodes():\n            return\n    \n        CH_indices, best_fitness = self.select_cluster_heads(round_num)\n        clusters = self.form_clusters(CH_indices)\n        self.stats['clusters_per_round'].append({\n            'round': round_num,\n            'CH': CH_indices.copy(),\n            'clusters': {ch: nodes.copy() for ch, nodes in clusters.items()}\n        })\n\n        E_consumed = self.wsn.calculate_energy_consumption(CH_indices, clusters)\n        self.wsn.update_energy(E_consumed)\n    \n        # Update statistics\n        alive_now = self.wsn.get_alive_nodes()\n        self.stats['total_energy'].append(E_consumed.sum())\n        self.stats['avg_energy'].append(np.mean([n.energy for n in alive_now]) if alive_now else 0)\n        self.stats['alive_nodes'].append(len(alive_now) / self.net_params.n_nodes * 100)\n        self.stats['best_fitness'].append(best_fitness)\n        self.stats['n_clusters'].append(len(CH_indices))\n        new_cumulative_energy = self.stats['cumulative_energy'][-1] + E_consumed.sum()\n        self.stats['cumulative_energy'].append(new_cumulative_energy)\n        dead_this_round = [i for i, node in enumerate(self.wsn.nodes) \n                          if node.energy == 0 and E_consumed[i] > 0]\n        self.stats['dead_rounds'].extend([round_num] * len(dead_this_round))\n        \n        # Visualization m·ªói N rounds\n        if round_num % visualize_interval == 0:\n            visualize_clusters_and_multihop(\n                self.wsn, CH_indices, clusters, self.net_params,\n                round_num, scenario_name, algorithm_name, output_dir\n            )\n\n    def run_round(self, round_num: int):\n        if not self.wsn.get_alive_nodes():\n            return\n\n        # alive_nows = self.wsn.get_alive_nodes()\n        # alive_nodes = len(alive_nows)\n        \n        # if not alive_nodes or len(alive_nodes) <= 1:\n        #     print(f\"üõë Network dead at round {round_num}\")\n        #     return False\n\n        # # 2. Functional death (WSN standard)\n        # min_alive_ratio = 0.05   # 5%\n        # if alive_nodes / self.net_params.n_nodes < min_alive_ratio:\n        #     print(f\"üõë Network functionally dead \"\n        #           f\"({alive_count}/{self.net_params.n_nodes}) at round {round_num}\")\n        #     return False\n\n        alive_nodes = self.wsn.get_alive_nodes()\n        alive_counts = len(alive_nodes)\n        if not alive_nodes or len(alive_nodes) <= 1:\n            print(f\"üõë Network dead at round {round_num}\")\n            return False\n        # 2. Functional death (WSN standard)\n        min_alive_ratio = 0.05   # 5%\n        if alive_counts / self.net_params.n_nodes < min_alive_ratio:\n            print(f\"üõë Network functionally dead \"\n                  f\"({alive_counts}/{self.net_params.n_nodes}) at round {round_num}\")\n            return False\n\n        \n        CH_indices, best_fitness = self.select_cluster_heads(round_num)\n    \n        # üõë STOP if fitness <= 0\n        if best_fitness <= 0:\n            print(f\"üõë Fitness <= 0 at round {round_num}. WSN DEAD.\")\n        \n            alive_count = len(self.wsn.get_alive_nodes())\n            dead_count  = self.net_params.n_nodes - alive_count\n        \n            self.stats.setdefault('alive_nodes', []).append(alive_count)\n            self.stats.setdefault('dead_nodes', []).append(dead_count)\n            self.stats.setdefault('best_fitness', []).append(best_fitness)\n            self.stats.setdefault('n_clusters', []).append(0)\n            self.stats.setdefault('total_energy', []).append(0.0)\n            self.stats.setdefault('avg_energy', []).append(0.0)\n            self.stats.setdefault('cumulative_energy', [0])\n            self.stats['cumulative_energy'].append(self.stats['cumulative_energy'][-1])\n        \n            return False   # ‚úÖ B·∫ÆT BU·ªòC\n        if best_fitness <= 0:\n            print(f\"üõë No feasible clustering (fitness={best_fitness:.4f}) \"\n                  f\"at round {round_num}\")\n            return False\n    \n        clusters = self.form_clusters(CH_indices)\n        clusters_clean = {int(k): [int(m) for m in v] for k,v in clusters.items()}\n    \n        if 'clusters_per_round' not in self.stats:\n            self.stats['clusters_per_round'] = []\n        self.stats['clusters_per_round'].append({\n            \"round\": round_num,\n            \"clusters\": clusters_clean\n        })\n    \n        # --- C·∫≠p nh·∫≠t nƒÉng l∆∞·ª£ng v√† alive nodes ---\n        E_consumed = self.wsn.calculate_energy_consumption(CH_indices, clusters)\n        self.wsn.update_energy(E_consumed)\n        alive_now = self.wsn.get_alive_nodes()\n\n        alive_count = len(alive_now)\n        dead_count  = self.net_params.n_nodes - alive_count\n        \n        # Kh·ªüi t·∫°o c√°c stats n·∫øu ch∆∞a t·ªìn t·∫°i\n        self.stats.setdefault('total_energy', [])\n        self.stats.setdefault('avg_energy', [])\n        self.stats.setdefault('alive_nodes', [])\n        self.stats.setdefault('best_fitness', [])\n        self.stats.setdefault('n_clusters', [])\n        self.stats.setdefault('cumulative_energy', [0])\n        self.stats.setdefault('dead_nodes', [0])\n        self.stats.setdefault('dead_rounds', [])\n    \n        # Append/update stats \n        self.stats['total_energy'].append(E_consumed.sum())\n        self.stats['avg_energy'].append(np.mean([n.energy for n in alive_now]) if alive_now else 0)\n\n        \n        self.stats['alive_nodes'].append(len(alive_now) / self.net_params.n_nodes * 100)\n\n        prev_dead = self.stats.get('prev_dead_count', 0)\n        current_dead = sum(1 for n in self.wsn.nodes if n.energy <= 0)\n        \n        new_dead = current_dead - prev_dead\n        \n        if new_dead > 0:\n            self.stats['dead_rounds'].extend([round_num] * new_dead)\n        \n        self.stats['prev_dead_count'] = current_dead\n        # self.stats['dead_nodes'].append(dead_count)\n        self.stats['best_fitness'].append(best_fitness)\n        self.stats['n_clusters'].append(len(CH_indices))\n        self.stats['cumulative_energy'].append(self.stats['cumulative_energy'][-1] + E_consumed.sum())\n\n\n\n    \n        # dead_this_round = [i for i, node in enumerate(self.wsn.nodes) if node.energy == 0 and E_consumed[i] > 0]\n        # self.stats['dead_rounds'].extend([round_num] * len(dead_this_round))\n    \n        # --- Save CSV ---\n        self.file_manager.save_round_clustering(\n            scenario_name=self.scenario_name,\n            algorithm_name=self.algorithm_name,\n            fitness_type=self.fitness_type,\n            round_num=round_num,\n            clusters=clusters_clean\n        )\n    \n        self.file_manager.save_round_state(\n            scenario_name=self.scenario_name,\n            algorithm_name=self.algorithm_name,\n            fitness_type=self.fitness_type,\n            round_num=round_num,\n            alive_nodes=alive_count ,\n            dead_nodes=dead_count,\n            clusters=clusters_clean,\n            best_fitness=self.stats['best_fitness'][-1]\n        )\n\n\n    def run_simulation(self, algorithm_name: str, log_interval: int = 250):\n        print(f\"\\n{'='*80}\")\n        print(f\"Starting {algorithm_name} simulation...\")\n        print(f\"{'='*80}\")\n\n        alive = int((self.wsn.energy > 0).sum().item())\n        total_e = 0.0\n\n        for round_num in range(1, self.net_params.Tmax + 1):\n            cont = self.run_round(round_num)\n    \n            # ‚õî STOP simulation n·∫øu run_round b√°o d·ª´ng\n            if cont is False:\n                # self.stats['dead_rounds'] = round_num\n                print(f\"\\nüõë Simulation stopped at round {round_num}\")\n                break\n\n                # ‚úÖ lu√¥n c·∫≠p nh·∫≠t state hi·ªán t·∫°i\n            alive = int((self.wsn.energy > 0).sum().item())\n            if self.stats['cumulative_energy']:\n                total_e = self.stats['cumulative_energy'][-1].item()\n            \n            if round_num % log_interval == 0:\n                alive = int((self.wsn.energy > 0).sum().item())\n                avg_e = self.stats['avg_energy'][-1]\n                total_e = self.stats['cumulative_energy'][-1].item()\n                n_ch = self.stats['n_clusters'][-1]\n                fitness = self.stats['best_fitness'][-1]\n                # self.log_round(round_num)\n    \n                print(f\"\\nRound {round_num:4d}/{self.net_params.Tmax}\")\n                print(f\"  Alive Nodes    : {alive:3d} ({alive/self.net_params.n_nodes*100:5.1f}%)\")\n                print(f\"  Avg Energy     : {avg_e:.6f} J\")\n                print(f\"  Total Consumed : {total_e:.4f} J\")\n                print(f\"  Num Clusters   : {n_ch}\")\n                print(f\"  Best Fitness   : {fitness:.4f}\")\n\n        # self.finalize_simulation()\n        # ================= Finalize stats =================\n        if len(self.stats['cumulative_energy']) > 1:\n            self.stats['plot_cumulative_energy'] = (\n                torch.stack(self.stats['cumulative_energy'][1:])\n                .detach()\n                .cpu()\n                .numpy()\n            )\n        else:\n            self.stats['plot_cumulative_energy'] = np.array([])\n    \n        print(f\"\\n‚úì {algorithm_name} simulation complete!\")\n        print(f\"  Final alive nodes: {alive}/{self.net_params.n_nodes}\")\n        print(f\"  Total energy consumed: {total_e:.4f} J\")\n    \n        return self.stats\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.340428Z","iopub.execute_input":"2026-01-23T08:37:18.340783Z","iopub.status.idle":"2026-01-23T08:37:18.390060Z","shell.execute_reply.started":"2026-01-23T08:37:18.340748Z","shell.execute_reply":"2026-01-23T08:37:18.388965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n\n\n\n\n\n\n# class ACACO(BaseAlgorithm):\n#     def __init__(self, wsn: WSN, aco_params: ACOParams, file_manager, scenario_name, fitness_type='old'):\n#         super().__init__(wsn, wsn.params)\n\n#         self.file_manager = file_manager\n#         self.scenario_name = scenario_name\n#         self.algorithm_name = None\n#         self.params = aco_params\n\n\n        \n#         self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n#         # copy tau l√™n GPU\n#         self.tau = torch.full(\n#             (n, n),\n#             self.params.tau0,\n#             device=self.device,\n#             dtype=torch.float32\n#         )\n\n#         n = self.net_params.n_nodes\n#         # self.tau = np.ones((n, n)) * self.params.tau0\n#         self.x_chaos = np.random.rand()\n\n#         self.fitness_type = fitness_type \n        \n#         if fitness_type == 'old':\n#             self.evaluate_CH_fitness = self.evaluate_CH_fitness_old.__get__(self)\n#             self.calculate_k = ClusterCalculator.k_old\n#         else:\n#             self.evaluate_CH_fitness = self.evaluate_CH_fitness_new.__get__(self)\n#             # self.calculate_k = ClusterCalculator.k_opt_leach\n#             self.calculate_k = ClusterCalculator.k_old\n        \n\n            \n#     # ===================== Logistic chaotic map =====================\n#     def logistic_map(self) -> float:\n#         self.x_chaos = self.params.r * self.x_chaos * (1 - self.x_chaos)\n#         return self.x_chaos\n\n#     # ===================== Adaptive parameters =====================\n#     def adaptive_rho(self, round_num: int) -> float:\n#         t_ratio = round_num / self.net_params.Tmax\n#         return self.params.rho_max - t_ratio * (self.params.rho_max - self.params.rho_min)\n\n#     def adaptive_beta(self, round_num: int) -> float:\n#         return self.params.beta_min + (self.params.beta_max - self.params.beta_min) * \\\n#                (1 / (1 + np.exp(-self.params.k * (round_num - self.net_params.Tmax/2))))\n\n#     def adaptive_alpha_chaos(self, round_num: int) -> float:\n#         amin, amax = self.params.alpha_chaos_min, self.params.alpha_chaos_max\n#         E_ub = self.net_params.n_nodes * self.net_params.E0\n#         E_lb = 0.0\n#         E_total_consumed = self.stats['cumulative_energy'][-1]\n#         if E_ub <= E_lb:\n#             return amin\n#         energy_ratio = (E_total_consumed - E_lb) / (E_ub - E_lb)\n#         alpha = amin + (amax - amin) * energy_ratio\n#         return max(amin, min(amax, alpha))\n\n#     # ===================== Heuristic calculation =====================\n#     def calculate_heuristic(self, alive_nodes: list) -> np.ndarray:\n#         n = len(alive_nodes)\n#         eta = np.zeros((n, n))\n#         for i in range(n):\n#             for j in range(n):\n#                 if i != j:\n#                     dist = alive_nodes[i].distance_to(alive_nodes[j])\n#                     if dist > 0 and alive_nodes[j].energy > 0:\n#                         eta[i, j] = alive_nodes[j].energy / dist\n#         if eta.max() > 0:\n#             eta /= eta.max()\n#         return eta\n\n\n\n\n#     # ===================== Select CHs (all ants) =====================\n#     def select_cluster_heads(self, round_num: int):\n#         alive_nodes = self.wsn.get_alive_nodes()\n#         if not alive_nodes:\n#             return [], 0.0, [], []\n\n#         # n_CH = max(1, int(self.net_params.p_CH * len(alive_nodes)))\n#         n_CH = self.calculate_k(self.net_params, alive_nodes)\n\n#         beta = self.adaptive_beta(round_num)\n#         alpha_chaos = self.adaptive_alpha_chaos(round_num)\n#         eta = self.calculate_heuristic(alive_nodes)\n#         energies = np.array([node.energy for node in alive_nodes])\n\n#         best_CHs, best_fitness = [], -np.inf\n#         all_CH_paths, all_fitness = [], []\n\n#         # ===================== for each ant =====================\n#         for ant in range(self.params.m):\n#             current_CHs, available = [], list(range(len(alive_nodes)))\n#             while len(current_CHs) < n_CH and available:\n#                 probs = np.zeros(len(available))\n#                 for k, idx in enumerate(available):\n#                     ref = current_CHs[-1] if current_CHs else 0\n#                     node_idx = alive_nodes[idx].idx\n#                     ref_node_idx = alive_nodes[ref].idx\n#                     tau_val = self.tau[ref_node_idx, node_idx]\n#                     eta_val = eta[ref, idx]\n#                     energy_val = energies[idx]\n#                     chaos_perturb = alpha_chaos * (self.x_chaos )\n#                     probs[k] = (tau_val ** self.params.alpha) * \\\n#                                (eta_val ** beta) * \\\n#                                ((1/energy_val) ** self.params.gamma) \n#                 probs = np.maximum(probs, 0)\n#                 if probs.sum() == 0:\n#                     probs = np.ones_like(probs)\n#                 probs /= probs.sum()\n#                 selected_idx = np.random.choice(len(available), p=probs)\n#                 current_CHs.append(available[selected_idx])\n#                 available.pop(selected_idx)\n#                 self.logistic_map()\n\n#             fitness = self.evaluate_CH_fitness(alive_nodes, current_CHs)\n#             all_CH_paths.append(current_CHs)\n#             all_fitness.append(fitness)\n\n#             if fitness > best_fitness:\n#                 best_fitness = fitness\n#                 best_CHs = current_CHs\n\n#         CH_indices = [alive_nodes[i].idx for i in best_CHs]\n#         return CH_indices, best_fitness, all_CH_paths, all_fitness\n\n#     # ===================== Update pheromone (multi-ant + chaos) =====================\n#     def update_pheromone(self, all_paths, all_fitness, rho, alpha_chaos):\n#         # 1. Evaporation\n#         self.tau *= (1 - rho)\n\n#         # 2. M·ªçi con ki·∫øn r·∫£i pheromone ŒîœÑ_k = Q / L_k\n#         for path, fitness in zip(all_paths, all_fitness):\n#             if fitness <= 0:\n#                 continue\n#             delta_tau = self.params.Q / fitness\n#             for i in range(len(path)):\n#                 for j in range(i + 1, len(path)):\n#                     u, v = path[i], path[j]\n#                     self.tau[u, v] += delta_tau\n#                     self.tau[v, u] += delta_tau\n\n#         # 3. Chaos perturbation term (global)\n#         chaos_term = alpha_chaos * (self.x_chaos )\n#         self.tau += chaos_term\n#         self.logistic_map()\n\n#         # 4. Clip pheromone\n#         self.tau = np.clip(self.tau, 0.1, 10.0)\n\n\n\n\n\n#     # ===================== Run one round =====================\n#     def run_round(self, round_num: int):\n#         alive_now = self.wsn.get_alive_nodes()\n#         # ===================== Check alive nodes first =====================\n#         alive_now = self.wsn.get_alive_nodes()\n#         if not alive_now:\n#             print(f\"‚ö†Ô∏è All nodes dead at round {round_num}\")\n#             return\n    \n    \n#         if not alive_now:\n#             return\n\n#         rho = self.adaptive_rho(round_num)\n#         alpha_chaos = self.adaptive_alpha_chaos(round_num)\n\n#         # ===================== Select CHs (all ants) =====================\n#         CH_indices, best_fitness, all_paths, all_fitness = self.select_cluster_heads(round_num)\n\n#         # üõë STOP if fitness <= 0 (no valid clustering possible)\n#         if best_fitness <= 0:\n#             print(f\"‚ö†Ô∏è Fitness <= 0 at round {round_num}. WSN considered dead.\")\n#             self.stats['alive_nodes'].append(0)\n#             self.stats['best_fitness'].append(best_fitness)\n#             return\n\n#         # ===================== Form clusters =====================\n#         clusters = self.form_clusters(CH_indices)\n\n#         # --- DEBUG: normalize clusters keys/values to ints\n#         serializable_clusters = {}\n#         for ch, members in clusters.items():\n#             # ensure keys and members are plain ints\n#             ch_int = int(ch)\n#             members_int = [int(m) for m in members]\n#             serializable_clusters[ch_int] = members_int\n\n#         alive_count = sum(1 for node in self.wsn.nodes if node.energy > 0)\n#         dead_count  = sum(1 for node in self.wsn.nodes if node.energy == 0)\n\n#         # --- SAVE ROUND CLUSTERING (CSV)\n#         # --- SAVE ROUND CLUSTERING (CSV)\n#         try:\n#             self.file_manager.save_round_clustering(\n#                 scenario_name=self.scenario_name,\n#                 algorithm_name=self.algorithm_name or \"Basic ACO\",\n#                 fitness_type=self.fitness_type,\n#                 round_num=round_num,\n#                 clusters=serializable_clusters\n#             )\n        \n#             self.file_manager.save_round_state(\n#                 scenario_name=self.scenario_name,\n#                 algorithm_name=self.algorithm_name,\n#                 fitness_type=self.fitness_type,\n#                 round_num=round_num,\n#                 alive_nodes=alive_count ,\n#                 dead_nodes=dead_count,\n#                 clusters=serializable_clusters,\n#                 best_fitness=self.stats['best_fitness'][-1]\n#             )\n\n        \n#         except Exception as e:\n#             print(f\"[WARN] Failed saving clustering round {round_num}: {e}\")\n\n        \n#         # # optional debug when clusters empty\n#         # if alive_now and (len(serializable_clusters) == 0 or all(len(m)==0 for m in serializable_clusters.values())):\n#         #     print(f\"[DEBUG] Round {round_num}: clusters empty for {self.algorithm_name}\")\n\n\n\n#         # ===================== Energy consumption =====================\n#         E_consumed = self.wsn.calculate_energy_consumption(CH_indices, clusters)\n#         self.wsn.update_energy(E_consumed)\n#         alive_now = self.wsn.get_alive_nodes()\n#         if not alive_now:\n#             print(f\"‚ö†Ô∏è All nodes dead at round {round_num}\")\n#             return\n#         # ===================== Update pheromone =====================\n#         self.update_pheromone(all_paths, all_fitness, rho, alpha_chaos)\n\n#         # ===================== Update statistics =====================\n#         alive_now = self.wsn.get_alive_nodes()\n#         self.stats['total_energy'].append(E_consumed.sum())\n#         self.stats['avg_energy'].append(np.mean([n.energy for n in alive_now]) if alive_now else 0)\n#         self.stats['alive_nodes'].append(len(alive_now) / self.net_params.n_nodes * 100)\n#         self.stats['best_fitness'].append(best_fitness)\n#         self.stats['n_clusters'].append(len(CH_indices))\n#         new_cumulative_energy = self.stats['cumulative_energy'][-1] + E_consumed.sum()\n#         self.stats['cumulative_energy'].append(new_cumulative_energy)\n#         dead_this_round = [i for i, node in enumerate(self.wsn.nodes) if node.energy == 0 and E_consumed[i] > 0]\n#         self.stats['dead_rounds'].extend([round_num] * len(dead_this_round))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.391755Z","iopub.execute_input":"2026-01-23T08:37:18.392127Z","iopub.status.idle":"2026-01-23T08:37:18.422778Z","shell.execute_reply.started":"2026-01-23T08:37:18.392084Z","shell.execute_reply":"2026-01-23T08:37:18.421340Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\n\n# ============================================================================\n# AC-ACO (GPU SAFE)\n# ============================================================================\n\nclass ACACO(BaseAlgorithm):\n    def __init__(self, wsn: WSN, aco_params: ACOParams,\n                 file_manager, scenario_name, fitness_type='old'):\n        super().__init__(wsn, wsn.params)\n\n        self.file_manager = file_manager\n        self.scenario_name = scenario_name\n        self.algorithm_name = \"AC-ACO\"\n        self.params = aco_params\n        self.fitness_type = fitness_type\n\n        # ---------------- DEVICE ----------------\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # ---------------- SIZE ----------------\n        self.n = self.net_params.n_nodes\n\n        # ---------------- PHEROMONE (GPU) ----------------\n        self.tau = torch.full(\n            (self.n, self.n),\n            self.params.tau0,\n            device=self.device,\n            dtype=torch.float32\n        )\n\n        # ---------------- CHAOS ----------------\n        self.x_chaos = np.random.rand()\n\n        # ---------------- FITNESS MODE ----------------\n        if fitness_type == 'old':\n            self.evaluate_CH_fitness = self.evaluate_CH_fitness_old.__get__(self)\n            self.calculate_k = ClusterCalculator.k_old\n        else:\n            self.evaluate_CH_fitness = self.evaluate_CH_fitness_new.__get__(self)\n            self.calculate_k = ClusterCalculator.k_opt_leach\n\n    # =========================================================================\n    # CHAOTIC MAP\n    # =========================================================================\n    def logistic_map(self):\n        # , round_num\n        # r = self.adaptive_r_chaos(round_num)\n        self.x_chaos = self.params.r * self.x_chaos * (1 - self.x_chaos)\n        return self.x_chaos\n\n    # =========================================================================\n    # ADAPTIVE PARAMETERS\n    # =========================================================================\n    def adaptive_rho(self, round_num):\n        t = round_num / self.net_params.Tmax\n        return self.params.rho_max - t * (self.params.rho_max - self.params.rho_min)\n\n    # def adaptive_beta(self, round_num):\n    #     return self.params.beta_min + (self.params.beta_max - self.params.beta_min) * \\\n    #            (1 / (1 + np.exp(-self.params.k *\n    #            (round_num - self.net_params.Tmax / 2))))\n\n    def adaptive_beta(self, round_num):\n        z = -self.params.k * (round_num - self.net_params.Tmax / 2)\n        z = np.clip(z, -50, 50)   # ‚õî ch·ªëng overflow\n        return self.params.beta_min + \\\n               (self.params.beta_max - self.params.beta_min) * \\\n               (1 / (1 + np.exp(z)))\n\n\n    # def adaptive_alpha_chaos(self, round_num):\n    #     amin, amax = self.params.alpha_chaos_min, self.params.alpha_chaos_max\n    #     E_total = self.stats['cumulative_energy'][-1]\n    #     E_max = self.net_params.n_nodes * self.net_params.E0\n    #     ratio = min(max(E_total / E_max, 0), 1)\n    #     return amin + (amax - amin) * ratio\n\n    def adaptive_alpha_chaos(self, round_num: int) -> float:\n        amin, amax = self.params.alpha_chaos_min, self.params.alpha_chaos_max\n        E_ub = self.net_params.n_nodes * self.net_params.E0\n        E_lb = 0.0\n        E_total_consumed = self.stats['cumulative_energy'][-1]\n        if E_ub <= E_lb:\n            return amin\n        energy_ratio = (E_total_consumed - E_lb) / (E_ub - E_lb)\n        alpha = amin + (amax - amin) * energy_ratio\n        return max(amin, min(amax, alpha))\n\n    # def adaptive_r_chaos(self, round_num):\n    #     t = round_num / self.net_params.Tmax\n    #     return self.r_max - t * (self.r_max - self.r_min)\n\n\n    # =========================================================================\n    # HEURISTIC (GPU)\n    # =========================================================================\n    def calculate_heuristic(self, alive_nodes):\n        idx = torch.tensor([n.idx for n in alive_nodes], device=self.device)\n\n        x = self.wsn.x[idx]\n        y = self.wsn.y[idx]\n        energy = self.wsn.energy[idx]\n\n        dx = x[:, None] - x[None, :]\n        dy = y[:, None] - y[None, :]\n        dist = torch.sqrt(dx**2 + dy**2).clamp(min=1e-6)\n\n        eta = energy[None, :] / dist\n        eta.fill_diagonal_(0)\n\n        return eta / eta.max().clamp(min=1e-6)\n\n    def to_tensor_list(lst, device):\n        return [x if torch.is_tensor(x) else torch.tensor(x, device=device) for x in lst]\n\n\n    def transmission_energy(self, d, packet_size=1):\n        \"\"\"\n        Em(i,j) theo m√¥ h√¨nh radio LEACH\n        packet_size: chu·∫©n ho√° = 1 (v√¨ ch·ªâ so s√°nh t∆∞∆°ng ƒë·ªëi)\n        \"\"\"\n        E_elec = self.net_params.Eelec\n        E_fs = self.net_params.Efs\n        E_mp = self.net_params.Emp\n        d0 = np.sqrt(self.net_params.Efs / self.net_params.Emp)\n    \n        if d < d0:\n            return self.net_params.ctrl_packet_length*(E_elec * packet_size + E_fs * packet_size * d**2)\n        else:\n            return self.net_params.ctrl_packet_length*(E_elec * packet_size + E_mp * packet_size * d**4)\n\n\n    \n    # =========================================================================\n    # SELECT CLUSTER HEADS (MULTI-ANT)\n    # =========================================================================\n    \n    \n    def select_cluster_heads(self, round_num):\n        alive_nodes = self.wsn.get_alive_nodes()\n        if not alive_nodes:\n            return [], 0.0, [], []\n\n        k = self.calculate_k(self.net_params, alive_nodes)\n        beta = self.adaptive_beta(round_num)\n        alpha_chaos = self.adaptive_alpha_chaos(round_num)\n\n        eta = self.calculate_heuristic(alive_nodes)\n        energies = np.array([n.energy for n in alive_nodes])\n\n        best_fitness = -np.inf\n        best_path = []\n\n        all_paths = []\n        all_fitness = []\n\n        for _ in range(self.params.m):\n            path = []\n            available = list(range(len(alive_nodes)))\n\n            while len(path) < k and available:\n                probs = np.zeros(len(available))\n\n                ref = path[-1] if path else 0\n                ref_idx = alive_nodes[ref].idx\n\n                for i, idx in enumerate(available):\n                    node_idx = alive_nodes[idx].idx\n                    tau_val = float(self.tau[ref_idx, node_idx].item())\n                    eta_val = float(eta[ref, idx].item())\n                    energy_val = max(energies[idx], 1e-6)\n\n                    # kho·∫£ng c√°ch i ‚Üí j\n                    dx = self.wsn.x[ref_idx] - self.wsn.x[node_idx]\n                    dy = self.wsn.y[ref_idx] - self.wsn.y[node_idx]\n                    \n                    dist_ij = torch.sqrt(dx * dx + dy * dy) + 1e-6\n                    dist_ij = dist_ij.item()   # chuy·ªÉn sang float Python\n\n                    # nƒÉng l∆∞·ª£ng truy·ªÅn Em(i,j)\n                    Em_ij = self.transmission_energy(dist_ij)\n                    \n                    probs[i] = (tau_val ** self.params.alpha) * \\\n                               (eta_val ** beta) * \\\n                               ((1 / Em_ij) ** self.params.gamma)\n\n                # print(\"type(probs):\", type(probs))\n                # if isinstance(probs, np.ndarray):\n                #     print(\"dtype:\", probs.dtype, \"shape:\", probs.shape)\n                # else:\n                #     print(\"probs:\", probs)\n\n                # ===== Chaos perturbation (paper-consistent) =====\n                # ƒë·∫£m b·∫£o numeric\n                # ===== Chaos perturbation (paper-consistent) =====\n                probs = np.asarray(probs, dtype=np.float64)\n                \n                alpha_chaos = float(alpha_chaos)\n                chaos = float(self.logistic_map())\n\n                probs += alpha_chaos * chaos\n                probs[probs < 0] = 0.0\n                \n                s = probs.sum()\n                if s > 0:\n                    probs /= s\n                else:\n                    probs[:] = 1.0 / len(probs)\n\n                # probs = np.maximum(probs, 0)\n                # probs = probs / probs.sum() if probs.sum() > 0 else np.ones_like(probs) / len(probs)\n\n                chosen = np.random.choice(len(available), p=probs)\n                path.append(available.pop(chosen))\n                self.logistic_map()\n\n            fitness = self.evaluate_CH_fitness(alive_nodes, path)\n            all_paths.append(path)\n            all_fitness.append(fitness)\n\n            if fitness > best_fitness:\n                best_fitness = fitness\n                best_path = path\n\n        CH_indices = [alive_nodes[i].idx for i in best_path]\n        return CH_indices, best_fitness, all_paths, all_fitness\n\n    # =========================================================================\n    # PHEROMONE UPDATE (GPU SAFE)\n    # =========================================================================\n    # def update_pheromone(self, all_paths, all_fitness, rho, alpha_chaos):\n    #     tau = self.tau\n    #     tau.mul_(1 - rho)\n\n    #     for path, fitness in zip(all_paths, all_fitness):\n    #         if fitness <= 0 or len(path) < 2:\n    #             continue\n\n    #         path = torch.tensor(path, device=self.device)\n    #         delta = self.params.Q / fitness\n\n    #         i, j = torch.triu_indices(len(path), len(path), offset=1)\n    #         u = path[i]\n    #         v = path[j]\n\n    #         tau[u, v] += delta\n    #         tau[v, u] += delta\n\n    #     tau.add_(alpha_chaos * self.x_chaos)\n    #     self.logistic_map()\n    #     tau.clamp_(0.1, 10.0)\n\n    def update_pheromone(self, all_paths, rho,alpha_chaos):\n        tau = self.tau\n        tau.mul_(1 - rho)\n    \n        for path in all_paths:\n            if len(path) < 2:\n                continue\n    \n            # t√≠nh L_k\n            Lk = 0.0\n            for i in range(len(path) - 1):\n                u = path[i]\n                v = path[i + 1]\n                dx = self.wsn.x[u] - self.wsn.x[v]\n                dy = self.wsn.y[u] - self.wsn.y[v]\n                Lk += torch.sqrt(dx * dx + dy * dy)\n    \n            if Lk <= 0:\n                continue\n    \n            delta = self.params.Q / Lk\n    \n            for i in range(len(path) - 1):\n                u = path[i]\n                v = path[i + 1]\n                tau[u, v] += delta\n                tau[v, u] += delta\n        tau.add_(alpha_chaos * self.x_chaos)\n        self.logistic_map()\n        tau.clamp_(0.1, 10.0)\n\n\n    # =========================================================================\n    # RUN ONE ROUND\n    # =========================================================================\n    # def run_round(self, round_num):\n    #     alive_nodes = self.wsn.get_alive_nodes()\n    \n    #  # ===== STOP CONDITIONS =====\n    #     if not alive_nodes or len(alive_nodes) == 0:\n    #         print(f\"üõë Network dead (0 alive nodes) at round {round_num}\")\n    #         return False\n    \n    #     if len(alive_nodes) <= 1:\n    #         print(f\"üõë Network dead (‚â§1 alive node) at round {round_num}\")\n    #         return False\n    \n    #     # ================= Adaptive params =================\n    #     rho = self.adaptive_rho(round_num)\n    #     alpha_chaos = self.adaptive_alpha_chaos(round_num)\n    \n    #     CH_indices, best_fitness, all_paths, all_fitness = \\\n    #         self.select_cluster_heads(round_num)\n        \n    #     if best_fitness <= 0:\n    #         print(f\"üõë No feasible clustering (fitness = {best_fitness:.4f}) \"\n    #               f\"at round {round_num}\")\n    #         return False   # ‚¨ÖÔ∏è D·ª™NG H·∫≤N\n        \n    #     clusters = self.form_clusters(CH_indices)\n    \n    #     serializable_clusters = {\n    #         int(ch): [int(m) for m in mem]\n    #         for ch, mem in clusters.items()\n    #     }\n    \n    #     # alive_count = int((self.wsn.energy > 0).sum().item())\n    #     # dead_count = self.n - alive_count\n    \n    #     # ================= Save state =================\n    #     try:\n    #         self.file_manager.save_round_clustering(\n    #             self.scenario_name, self.algorithm_name,\n    #             self.fitness_type, round_num, serializable_clusters\n    #         )\n    \n    #         self.file_manager.save_round_state(\n    #             self.scenario_name, self.algorithm_name,\n    #             self.fitness_type, round_num,\n    #             alive_count, dead_count,\n    #             serializable_clusters,\n    #             best_fitness\n    #         )\n    #     except Exception as e:\n    #         print(f\"[WARN] Failed saving clustering round {round_num}: {e}\")\n    \n    #     # ================= Energy =================\n    #     E = self.wsn.calculate_energy_consumption(CH_indices, clusters)\n    #     self.wsn.update_energy(E)\n    \n    #     # ================= Pheromone =================\n    #     self.update_pheromone(all_paths, all_fitness, rho, alpha_chaos)\n\n\n    #     # ‚úÖ ALIVE COUNT PH·∫¢I SAU UPDATE\n    #     alive_count = int((self.wsn.energy > 0).sum().item())\n    #     dead_count = self.net_params.n_nodes - alive_count\n        \n        # # ================= Stats =================\n        # round_energy = E.sum()\n        # prev_cum = (\n        #     self.stats['cumulative_energy'][-1]\n        #     if self.stats['cumulative_energy']\n        #     else torch.tensor(0.0, device=self.device)\n        # )\n    \n        # cumulative_energy = prev_cum + round_energy\n    \n        # self.stats['total_energy'].append(round_energy.item())\n        # self.stats['cumulative_energy'].append(cumulative_energy)\n        # self.stats['avg_energy'].append(\n        #     self.wsn.energy[self.wsn.energy > 0].mean().item()\n        # )\n        # self.stats['alive_nodes'].append(alive_count / self.n * 100)\n        # self.stats['best_fitness'].append(best_fitness)\n        # self.stats['n_clusters'].append(len(CH_indices))\n    \n        # return True\n\n    \n    def run_round(self, round_num):\n        # ================= 1. Check alive =================\n        alive_nodes = self.wsn.get_alive_nodes()\n        alive_counts = len(alive_nodes)\n        if not alive_nodes or len(alive_nodes) <= 1:\n            print(f\"üõë Network dead at round {round_num}\")\n            return False\n        # 2. Functional death (WSN standard)\n        min_alive_ratio = 0.05   # 5%\n        if alive_counts / self.net_params.n_nodes < min_alive_ratio:\n            print(f\"üõë Network functionally dead \"\n                  f\"({alive_count}/{self.net_params.n_nodes}) at round {round_num}\")\n            return False\n        # ================= 2. Adaptive params =================\n        rho = self.adaptive_rho(round_num)\n        alpha_chaos = self.adaptive_alpha_chaos(round_num)\n    \n        # ================= 3. ACO selection =================\n        CH_indices, best_fitness, all_paths, all_fitness = \\\n            self.select_cluster_heads(round_num)\n    \n        if best_fitness <= 0:\n            print(f\"üõë No feasible clustering (fitness={best_fitness:.4f}) \"\n                  f\"at round {round_num}\")\n            return False\n    \n        # ================= 4. Form clusters =================\n        clusters = self.form_clusters(CH_indices)\n        serializable_clusters = {\n            int(ch): [int(m) for m in mem]\n            for ch, mem in clusters.items()\n        }\n    \n        # ================= 5. Energy =================\n        E = self.wsn.calculate_energy_consumption(CH_indices, clusters)\n        self.wsn.update_energy(E)\n    \n        # ================= 6. Recompute alive =================\n        alive_count = int((self.wsn.energy > 0).sum().item())\n\n        # 2. Functional death (WSN standard)\n        min_alive_ratio = 0.05   # 5%\n        if alive_count / self.net_params.n_nodes < min_alive_ratio:\n            print(f\"üõë Network functionally dead \"\n                  f\"({alive_count}/{self.net_params.n_nodes}) at round {round_num}\")\n            return False\n        \n        dead_count = self.net_params.n_nodes - alive_count\n    \n        # ================= 7. Save state =================\n        try:\n            self.file_manager.save_round_clustering(\n                self.scenario_name, self.algorithm_name,\n                self.fitness_type, round_num, serializable_clusters\n            )\n    \n            self.file_manager.save_round_state(\n                self.scenario_name, self.algorithm_name,\n                self.fitness_type, round_num,\n                alive_count, dead_count,\n                serializable_clusters,\n                best_fitness\n            )\n        except Exception as e:\n            print(f\"[WARN] Failed saving round {round_num}: {e}\")\n    \n        # ================= 8. Pheromone =================\n        self.update_pheromone(all_paths, rho, alpha_chaos)\n    \n        # ================= 9. Stats =================\n        round_energy = E.sum()\n        prev_cum = self.stats['cumulative_energy'][-1]\n    \n        self.stats['total_energy'].append(round_energy.item())\n        self.stats['cumulative_energy'].append(prev_cum + round_energy)\n    \n        avg_energy = (\n            self.wsn.energy[self.wsn.energy > 0].mean().item()\n            if alive_count > 0 else 0.0\n        )\n    \n        self.stats['avg_energy'].append(avg_energy)\n        self.stats['alive_nodes'].append(\n            alive_count / self.net_params.n_nodes * 100\n        )\n        self.stats['best_fitness'].append(best_fitness)\n        self.stats['n_clusters'].append(len(CH_indices))\n\n        # alive_now = self.wsn.get_alive_nodes()\n        # self.stats['total_energy'].append(E_consumed.sum())\n        # self.stats['avg_energy'].append(np.mean([n.energy for n in alive_now]) if alive_now else 0)\n        # self.stats['alive_nodes'].append(len(alive_now) / self.net_params.n_nodes * 100)\n        # self.stats['best_fitness'].append(best_fitness)\n        # self.stats['n_clusters'].append(len(CH_indices))\n        # new_cumulative_energy = self.stats['cumulative_energy'][-1] + E_consumed.sum()\n        # self.stats['cumulative_energy'].append(new_cumulative_energy)\n        # dead_this_round = [i for i, node in enumerate(self.wsn.nodes) if node.energy == 0 and E_consumed[i] > 0]\n        # self.stats['dead_rounds'].extend([round_num] * len(dead_this_round))\n\n \n        self.stats['clusters_per_round'].append({\n            'round': round_num,\n            'CH': CH_indices.copy(),\n            'clusters': {ch: nodes.copy() for ch, nodes in clusters.items()}\n        })\n        prev_dead = self.stats.get('prev_dead_count', 0)\n        current_dead = sum(1 for n in self.wsn.nodes if n.energy <= 0)\n        \n        new_dead = current_dead - prev_dead\n        \n        if new_dead > 0:\n            self.stats['dead_rounds'].extend([round_num] * new_dead)\n        \n        self.stats['prev_dead_count'] = current_dead\n            \n        return True\n\n\n  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.424469Z","iopub.execute_input":"2026-01-23T08:37:18.424793Z","iopub.status.idle":"2026-01-23T08:37:18.478576Z","shell.execute_reply.started":"2026-01-23T08:37:18.424764Z","shell.execute_reply":"2026-01-23T08:37:18.477463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class BasicACO(BaseAlgorithm):\n#     def __init__(self, wsn: WSN, aco_params: ACOParams, file_manager, scenario_name, fitness_type='old'):\n#         super().__init__(wsn, wsn.params)\n#         self.file_manager = file_manager\n#         self.scenario_name = scenario_name\n#         self.algorithm_name = None  # s·∫Ω ƒë∆∞·ª£c set t·ª´ ngo√†i (run_scenario_comparison)\n#         self.params = aco_params\n#         n = self.net_params.n_nodes\n#         self.tau = np.ones((n, n)) * self.params.tau0\n#         self.rho = 0.5\n        \n\n#         # --------------------------------------------------\n#         #  Mapping FITNESS + CLUSTER COUNT METHOD\n#         # --------------------------------------------------\n#         if fitness_type == 'old':\n#             self.evaluate_CH_fitness = self.evaluate_CH_fitness_old.__get__(self)\n#             self.calculate_k = ClusterCalculator.k_old\n#         else:\n#             self.evaluate_CH_fitness = self.evaluate_CH_fitness_new.__get__(self)\n#             # self.calculate_k = ClusterCalculator.k_opt_leach\n#             self.calculate_k = ClusterCalculator.k_old\n\n\n#     def calculate_heuristic(self, alive_nodes: List[Node]) -> np.ndarray:\n#         n = len(alive_nodes)\n#         eta = np.zeros((n, n))\n#         for i in range(n):\n#             for j in range(n):\n#                 if i != j:\n#                     dist = alive_nodes[i].distance_to(alive_nodes[j])\n#                     if dist > 0:\n#                         eta[i, j] = 1.0 / dist\n#         if eta.max() > 0:\n#             eta = eta / eta.max()\n#         return eta\n\n\n\n\n#     # ===================== SELECT CHs (tr·∫£ v·ªÅ ALL paths & ALL fitness) =====================\n#     def select_cluster_heads(self, round_num: int):\n#         alive_nodes = self.wsn.get_alive_nodes()\n#         if not alive_nodes:\n#             return [], [], [], []\n\n#         # n_CH = max(1, int(self.net_params.p_CH * len(alive_nodes)))\n#         n_CH = self.calculate_k(self.net_params, alive_nodes)\n\n#         eta = self.calculate_heuristic(alive_nodes)\n#         energies = np.array([node.energy for node in alive_nodes])\n\n#         all_paths, all_fitness = [], []\n#         best_CHs, best_fitness = [], -np.inf\n\n#         for ant in range(self.params.m):\n#             current_CHs, available = [], list(range(len(alive_nodes)))\n\n#             while len(current_CHs) < n_CH and available:\n#                 probs = np.zeros(len(available))\n\n#                 for k, idx in enumerate(available):\n#                     ref = current_CHs[-1] if current_CHs else 0\n#                     node_idx = alive_nodes[idx].idx\n#                     ref_node_idx = alive_nodes[ref].idx\n#                     tau_val = self.tau[ref_node_idx, node_idx]\n#                     eta_val = eta[ref, idx]\n#                     energy_val = energies[idx]\n\n#                     probs[k] = (tau_val ** self.params.alpha) * \\\n#                                (eta_val ** self.params.beta_init) \n#                 probs = np.maximum(probs, 0)\n#                 if probs.sum() == 0:\n#                     probs = np.ones_like(probs)\n#                 probs /= probs.sum()\n\n#                 selected_idx = np.random.choice(len(available), p=probs)\n#                 current_CHs.append(available[selected_idx])\n#                 available.pop(selected_idx)\n\n#             fitness = self.evaluate_CH_fitness(alive_nodes, current_CHs)\n#             all_paths.append(current_CHs)\n#             all_fitness.append(fitness)\n\n#             if fitness > best_fitness:\n#                 best_fitness = fitness\n#                 best_CHs = current_CHs\n\n#         CH_indices = [alive_nodes[i].idx for i in best_CHs]\n#         return CH_indices, best_fitness, all_paths, all_fitness\n\n#     # ===================== MULTI-ANT PHEROMONE UPDATE =====================\n#     def update_pheromone(self, all_paths, all_fitness):\n#         # 1. Evaporation\n#         self.tau *= (1 - self.rho)\n\n#         # 2. ALL ANTS DEPOSIT PHEROMONE\n#         for path, fitness in zip(all_paths, all_fitness):\n#             if fitness <= 0:\n#                 continue\n#             delta_tau = self.params.Q / fitness\n#             for i in range(len(path)):\n#                 for j in range(i + 1, len(path)):\n#                     u, v = path[i], path[j]\n#                     self.tau[u, v] += delta_tau\n#                     self.tau[v, u] += delta_tau\n#         # 3. Clip\n#         self.tau = np.clip(self.tau, 0.1, 10.0)\n\n\n\n\n#     # ===================== RUN ONE ROUND =====================\n#     def run_round(self, round_num: int):\n#         alive_now = self.wsn.get_alive_nodes()\n#         # ===================== Check alive nodes first =====================\n\n        \n        \n#         if not alive_now:\n#             print(f\"‚ö†Ô∏è All nodes dead at round {round_num}\")\n#             return\n\n#         if not alive_now:   \n#             return\n\n#         # --- ALL ANTS SELECTION ---\n#         CH_indices, best_fitness, all_paths, all_fitness = \\\n#             self.select_cluster_heads(round_num)\n\n#         # üõë STOP if fitness <= 0 (no valid clustering possible)\n#         if best_fitness <= 0:\n#             print(f\"‚ö†Ô∏è Fitness <= 0 at round {round_num}. WSN considered dead.\")\n#             self.stats['alive_nodes'].append(0)\n#             self.stats['best_fitness'].append(best_fitness)\n#             return\n\n        \n#         clusters = self.form_clusters(CH_indices)\n\n#                 # --- DEBUG: normalize clusters keys/values to ints\n#         serializable_clusters = {}\n#         for ch, members in clusters.items():\n#             # ensure keys and members are plain ints\n#             ch_int = int(ch)\n#             members_int = [int(m) for m in members]\n#             serializable_clusters[ch_int] = members_int\n#         alive_count = sum(1 for node in self.wsn.nodes if node.energy > 0)\n#         dead_count  = sum(1 for node in self.wsn.nodes if node.energy == 0)\n\n#         # --- SAVE ROUND CLUSTERING (CSV)\n#         # --- SAVE ROUND CLUSTERING (CSV)\n#         if not serializable_clusters or all(len(m) == 0 for m in serializable_clusters.values()):\n#             print(f\"[WARN] Skip saving clustering round {round_num}: empty clusters\")\n#         else:\n\n#                 try:\n#                     self.file_manager.save_round_clustering(\n#                         scenario_name=self.scenario_name,\n#                         algorithm_name=self.algorithm_name or \"Basic ACO\",\n#                         fitness_type=self.fitness_type,\n#                         round_num=round_num,\n#                         clusters=serializable_clusters\n#                     )\n                \n#                     self.file_manager.save_round_state(\n#                         scenario_name=self.scenario_name,\n#                         algorithm_name=self.algorithm_name,\n#                         fitness_type=self.fitness_type,\n#                         round_num=round_num,\n#                         alive_nodes=alive_count ,\n#                         dead_nodes=dead_count,\n#                         clusters=serializable_clusters,\n#                         best_fitness=self.stats['best_fitness'][-1]\n#                     )\n        \n                \n#                 except Exception as e:\n#                     print(f\"[WARN] Failed saving clustering round {round_num}: {e}\")\n\n        \n#         # # optional debug when clusters empty\n#         # if alive_now and (len(serializable_clusters) == 0 or all(len(m)==0 for m in serializable_clusters.values())):\n#         #     print(f\"[DEBUG] Round {round_num}: clusters empty for {self.algorithm_name}\")\n\n\n#         E_consumed = self.wsn.calculate_energy_consumption(CH_indices, clusters)\n#         self.wsn.update_energy(E_consumed)\n#         alive_now = self.wsn.get_alive_nodes()\n#         if not alive_now:\n#             print(f\"‚ö†Ô∏è All nodes dead at round {round_num}\")\n#             return\n\n#         # --- MULTI-ANT PHEROMONE UPDATE ---\n#         self.update_pheromone(all_paths, all_fitness)\n\n#         # --- STATS ---\n#         alive_now = self.wsn.get_alive_nodes()\n#         self.stats['total_energy'].append(E_consumed.sum())\n#         self.stats['avg_energy'].append(np.mean([n.energy for n in alive_now]) if alive_now else 0)\n#         self.stats['alive_nodes'].append(len(alive_now) / self.net_params.n_nodes * 100)\n#         self.stats['best_fitness'].append(best_fitness)\n#         self.stats['n_clusters'].append(len(CH_indices))\n#         new_E = self.stats['cumulative_energy'][-1] + E_consumed.sum()\n#         self.stats['cumulative_energy'].append(new_E)\n#         dead_round = [i for i, n in enumerate(self.wsn.nodes) if n.energy == 0 and E_consumed[i] > 0]\n#         self.stats['dead_rounds'].extend([round_num] * len(dead_round))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.482693Z","iopub.execute_input":"2026-01-23T08:37:18.483066Z","iopub.status.idle":"2026-01-23T08:37:18.504906Z","shell.execute_reply.started":"2026-01-23T08:37:18.483024Z","shell.execute_reply":"2026-01-23T08:37:18.503599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\n\n# ============================================================================\n# BASIC ACO (GPU VERSION)\n# ============================================================================\n\nclass BasicACO(BaseAlgorithm):\n    def __init__(self, wsn: WSN, aco_params: ACOParams,\n                 file_manager, scenario_name, fitness_type='old'):\n        super().__init__(wsn, wsn.params)\n\n        self.file_manager = file_manager\n        self.scenario_name = scenario_name\n        self.algorithm_name = \"Basic ACO\"\n        self.params = aco_params\n        self.fitness_type = fitness_type\n\n        # ---------------- DEVICE ----------------\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # ---------------- SIZE ----------------\n        self.n = self.net_params.n_nodes\n\n        # ---------------- PHEROMONE (GPU) ----------------\n        self.tau = torch.full(\n            (self.n, self.n),\n            self.params.tau0,\n            device=self.device,\n            dtype=torch.float32\n        )\n\n        self.rho = 0.5\n\n        # ---------------- FITNESS MODE ----------------\n        if fitness_type == 'old':\n            self.evaluate_CH_fitness = self.evaluate_CH_fitness_old.__get__(self)\n            self.calculate_k = ClusterCalculator.k_old\n        else:\n            self.evaluate_CH_fitness = self.evaluate_CH_fitness_new.__get__(self)\n            self.calculate_k = ClusterCalculator.k_opt_leach\n\n    # =========================================================================\n    # HEURISTIC (GPU)\n    # =========================================================================\n    def calculate_heuristic(self, alive_nodes):\n        idx = torch.tensor([n.idx for n in alive_nodes], device=self.device)\n\n        x = self.wsn.x[idx]\n        y = self.wsn.y[idx]\n\n        dx = x[:, None] - x[None, :]\n        dy = y[:, None] - y[None, :]\n        dist = torch.sqrt(dx**2 + dy**2).clamp(min=1e-6)\n\n        eta = 1.0 / dist\n        eta.fill_diagonal_(0)\n\n        return eta / eta.max().clamp(min=1e-6)\n\n    # =========================================================================\n    # SELECT CLUSTER HEADS (MULTI-ANT)\n    # =========================================================================\n    def select_cluster_heads(self, round_num):\n        alive_nodes = self.wsn.get_alive_nodes()\n        if not alive_nodes:\n            return [], 0.0, [], []\n\n        k = self.calculate_k(self.net_params, alive_nodes)\n        eta = self.calculate_heuristic(alive_nodes)\n\n        energies = torch.tensor(\n            [n.energy for n in alive_nodes],\n            device=self.device\n        ).clamp(min=1e-6)\n\n        best_fitness = -float(\"inf\")\n        best_path = []\n\n        all_paths = []\n        all_fitness = []\n\n        for _ in range(self.params.m):\n            path = []\n            available = list(range(len(alive_nodes)))\n\n            while len(path) < k and available:\n                probs = []\n\n                ref = path[-1] if path else 0\n                ref_idx = alive_nodes[ref].idx\n\n                for idx in available:\n                    node_idx = alive_nodes[idx].idx\n                    tau_val = self.tau[ref_idx, node_idx]\n                    eta_val = eta[ref, idx]\n                    probs.append(\n                        (tau_val ** self.params.alpha) *\n                        (eta_val ** self.params.beta_init)\n                    )\n\n                probs = torch.stack(probs)\n                probs = torch.clamp(probs, min=0)\n\n                if probs.sum() == 0:\n                    probs = torch.ones_like(probs)\n\n                probs = probs / probs.sum()\n                choice = torch.multinomial(probs, 1).item()\n\n                path.append(available.pop(choice))\n\n            fitness = self.evaluate_CH_fitness(alive_nodes, path)\n            all_paths.append(path)\n            all_fitness.append(fitness)\n\n            if fitness > best_fitness:\n                best_fitness = fitness\n                best_path = path\n\n        CH_indices = [alive_nodes[i].idx for i in best_path]\n        return CH_indices, best_fitness, all_paths, all_fitness\n\n    # =========================================================================\n    # PHEROMONE UPDATE (GPU SAFE)\n    # =========================================================================\n    # def update_pheromone(self, all_paths, all_fitness):\n    #     tau = self.tau\n    #     tau.mul_(1 - self.rho)\n\n    #     for path, fitness in zip(all_paths, all_fitness):\n    #         if fitness <= 0 or len(path) < 2:\n    #             continue\n\n    #         path = torch.tensor(path, device=self.device)\n    #         delta = self.params.Q / fitness\n\n    #         i, j = torch.triu_indices(len(path), len(path), offset=1)\n    #         u = path[i]\n    #         v = path[j]\n\n    #         tau[u, v] += delta\n    #         tau[v, u] += delta\n\n    #     tau.clamp_(0.1, 10.0)\n\n    def update_pheromone(self, all_paths):\n        tau = self.tau\n        tau.mul_(1 - self.rho)\n    \n        for path in all_paths:\n            if len(path) < 2:\n                continue\n    \n            Lk = 0.0\n            for i in range(len(path) - 1):\n                u, v = path[i], path[i+1]\n                dx = self.wsn.x[u] - self.wsn.x[v]\n                dy = self.wsn.y[u] - self.wsn.y[v]\n                Lk += torch.sqrt(dx*dx + dy*dy)\n    \n            Lk = Lk.item()\n            if Lk <= 0:\n                continue\n    \n            delta = self.params.Q / Lk\n    \n            for i in range(len(path) - 1):\n                u, v = path[i], path[i+1]\n                tau[u, v] += delta\n                tau[v, u] += delta\n        \n        tau.clamp_(0.1, 10.0)\n\n\n    # =========================================================================\n    # RUN ONE ROUND\n    # =========================================================================\n    # def run_round(self, round_num):\n    #     # ================= 1. Check alive =================\n    #     alive_nodes = self.wsn.get_alive_nodes()\n    #     if not alive_nodes:\n    #         print(f\"‚ö†Ô∏è All nodes dead at round {round_num}\")\n    #         return False\n    \n    #     # ================= 2. ACO selection =================\n    #     CH_indices, best_fitness, all_paths, all_fitness = \\\n    #         self.select_cluster_heads(round_num)\n    \n    #     if best_fitness <= 0:\n    #         self.stats['alive_nodes'].append(0)\n    #         self.stats['best_fitness'].append(best_fitness)\n    #         return False\n    \n    #     # ================= 3. Form clusters =================\n    #     clusters = self.form_clusters(CH_indices)\n    \n    #     serializable_clusters = {\n    #         int(ch): [int(m) for m in mem]\n    #         for ch, mem in clusters.items()\n    #     }\n    \n   \n\n    #     # ================= 5. Energy =================\n    #     E = self.wsn.calculate_energy_consumption(CH_indices, clusters)\n    #     self.wsn.update_energy(E)\n\n\n\n    #     # ‚úÖ ALIVE COUNT PH·∫¢I SAU UPDATE\n    #     alive_count = int((self.wsn.energy > 0).sum().item())\n    #     dead_count = self.net_params.n_nodes - alive_count\n\n\n    \n    #     # ================= 4. Save state =================\n    #     if serializable_clusters and any(len(v) > 0 for v in serializable_clusters.values()):\n    #         try:\n    #             self.file_manager.save_round_clustering(\n    #                 self.scenario_name, self.algorithm_name,\n    #                 self.fitness_type, round_num,\n    #                 serializable_clusters\n    #             )\n    \n    #             self.file_manager.save_round_state(\n    #                 self.scenario_name, self.algorithm_name,\n    #                 self.fitness_type, round_num,\n    #                 alive_count, dead_count,\n    #                 serializable_clusters,\n    #                 best_fitness\n    #             )\n    #         except Exception as e:\n        #         print(f\"[WARN] Failed saving clustering round {round_num}: {e}\")\n    \n        \n        # # ================= 6. Stop if dead =================\n        # if not self.wsn.get_alive_nodes():\n        #     print(f\"‚ö†Ô∏è All nodes dead at round {round_num}\")\n        #     return False\n    \n        # # ================= 7. Pheromone =================\n        # self.update_pheromone(all_paths, all_fitness)\n    \n        # # ================= 8. Stats =================\n        # round_energy = E.sum()\n    \n        # prev_cum = (\n        #     self.stats['cumulative_energy'][-1]\n        #     if self.stats['cumulative_energy']\n        #     else torch.tensor(0.0, device=self.device)\n        # )\n    \n        # self.stats['total_energy'].append(round_energy.item())\n        # self.stats['cumulative_energy'].append(prev_cum + round_energy)\n        # self.stats['avg_energy'].append(\n        #     self.wsn.energy[self.wsn.energy > 0].mean().item()\n        # )\n        # self.stats['alive_nodes'].append(alive_count / self.n * 100)\n        # self.stats['best_fitness'].append(best_fitness)\n        # self.stats['n_clusters'].append(len(CH_indices))\n    \n        # return True\n\n\n\n\n    def run_round(self, round_num):\n        # ================= 1. Check alive =================\n        alive_nodes = self.wsn.get_alive_nodes()\n        alive_counts = len(alive_nodes)\n        if not alive_nodes:\n            print(f\"üõë All nodes dead at round {round_num}\")\n            return False\n\n        # 2. Functional death (WSN standard)\n        min_alive_ratio = 0.05   # 5%\n        if alive_counts / self.net_params.n_nodes < min_alive_ratio:\n            print(f\"üõë Network functionally dead \"\n                  f\"({alive_counts}/{self.net_params.n_nodes}) at round {round_num}\")\n            return False\n    \n        # ================= 2. ACO selection =================\n        CH_indices, best_fitness, all_paths, all_fitness = \\\n            self.select_cluster_heads(round_num)\n    \n        if best_fitness <= 0:\n            print(f\"üõë No feasible clustering (fitness={best_fitness:.4f}) \"\n                  f\"at round {round_num}\")\n    \n            # ghi ƒë·ªß stats ƒë·ªÉ kh√¥ng l·ªách\n            prev_cum = self.stats['cumulative_energy'][-1]\n            self.stats['total_energy'].append(0.0)\n            self.stats['cumulative_energy'].append(prev_cum)\n            self.stats['avg_energy'].append(0.0)\n            self.stats['alive_nodes'].append(0.0)\n            self.stats['best_fitness'].append(best_fitness)\n            self.stats['n_clusters'].append(0)\n    \n            return False\n    \n        # ================= 3. Form clusters =================\n        clusters = self.form_clusters(CH_indices)\n        serializable_clusters = {\n            int(ch): [int(m) for m in mem]\n            for ch, mem in clusters.items()\n        }\n    \n        # ================= 4. Energy =================\n        E = self.wsn.calculate_energy_consumption(CH_indices, clusters)\n        self.wsn.update_energy(E)\n    \n        # ================= 5. Recompute alive =================\n        alive_count = int((self.wsn.energy > 0).sum().item())\n        dead_count = self.net_params.n_nodes - alive_count\n        \n        # ================= 6. Save state =================\n        try:\n            self.file_manager.save_round_clustering(\n                self.scenario_name, self.algorithm_name,\n                self.fitness_type, round_num,\n                serializable_clusters\n            )\n    \n            self.file_manager.save_round_state(\n                self.scenario_name, self.algorithm_name,\n                self.fitness_type, round_num,\n                alive_count, dead_count,\n                serializable_clusters,\n                best_fitness\n            )\n        except Exception as e:\n            print(f\"[WARN] Failed saving round {round_num}: {e}\")\n    \n        # ================= 7. Pheromone =================\n        self.update_pheromone(all_paths)\n    \n        # # ================= 8. Stats =================\n        round_energy = E.sum()\n        prev_cum = self.stats['cumulative_energy'][-1]\n    \n        self.stats['total_energy'].append(round_energy.item())\n        self.stats['cumulative_energy'].append(prev_cum + round_energy)\n    \n        avg_energy = (\n            self.wsn.energy[self.wsn.energy > 0].mean().item()\n            if alive_count > 0 else 0.0\n        )\n    \n        self.stats['avg_energy'].append(avg_energy)\n        self.stats['alive_nodes'].append(\n            alive_count / self.net_params.n_nodes * 100\n        )\n        self.stats['best_fitness'].append(best_fitness)\n        self.stats['n_clusters'].append(len(CH_indices))\n\n                # --- STATS ---\n        alive_now = self.wsn.get_alive_nodes()\n        # self.stats['total_energy'].append(E_consumed.sum())\n        # self.stats['avg_energy'].append(np.mean(1[n.energy for n in alive_now]) if alive_now else 0)\n        # self.stats['alive_nodes'].append(len(alive_now) / self.net_params.n_nodes * 100)\n        # self.stats['best_fitness'].append(best_fitness)\n        # self.stats['n_clusters'].append(len(CH_indices))\n        # new_E = self.stats['cumulative_energy'][-1] + E_consumed.sum()\n        # self.stats['cumulative_energy'].append(new_E)\n        # dead_round = [i for i, n in enumerate(self.wsn.nodes) if n.energy == 0 and E_consumed[i] > 0]\n        # self.stats['dead_rounds'].extend([round_num] * len(dead_round))\n        self.stats['clusters_per_round'].append({\n            'round': round_num,\n            'CH': CH_indices.copy(),\n            'clusters': {ch: nodes.copy() for ch, nodes in clusters.items()}\n        })\n\n        prev_dead = self.stats.get('prev_dead_count', 0)\n        current_dead = sum(1 for n in self.wsn.nodes if n.energy <= 0)\n        \n        new_dead = current_dead - prev_dead\n        \n        if new_dead > 0:\n            self.stats['dead_rounds'].extend([round_num] * new_dead)\n        \n        self.stats['prev_dead_count'] = current_dead\n            \n    \n        # ================= 9. Stop if dead =================\n        if alive_count == 0:\n            print(f\"üõë Network died at end of round {round_num}\")\n            return False\n    \n        return True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.506539Z","iopub.execute_input":"2026-01-23T08:37:18.506804Z","iopub.status.idle":"2026-01-23T08:37:18.542568Z","shell.execute_reply.started":"2026-01-23T08:37:18.506778Z","shell.execute_reply":"2026-01-23T08:37:18.541390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# GENETIC ALGORITHM\n# ============================================================================\nimport torch\n\n\n\nclass GeneticAlgorithm(BaseAlgorithm):\n    def __init__(self, wsn: WSN, ga_params: GAParams, net_params: NetworkParams,\n                 fitness_type='old', file_manager=None, scenario_name=None):\n\n        super().__init__(wsn, net_params)\n        \n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.params = ga_params\n        self.fitness_type = fitness_type\n        self.file_manager = file_manager\n        self.scenario_name = scenario_name\n\n        # --------------------------------------------------\n        #  Mapping FITNESS + CLUSTER COUNT METHOD\n        # --------------------------------------------------\n        if fitness_type == 'old':\n            self.evaluate_CH_fitness = self.evaluate_CH_fitness_old.__get__(self)\n            self.calculate_k = ClusterCalculator.k_old\n        else:\n            self.evaluate_CH_fitness = self.evaluate_CH_fitness_new.__get__(self)\n            self.calculate_k = ClusterCalculator.k_opt_leach\n            # self.calculate_k = ClusterCalculator.k_old\n\n\n    def create_chromosome(self, n_nodes: int, n_CH: int) -> np.ndarray:\n        n_CH = min(n_CH, n_nodes)  # ƒë·∫£m b·∫£o kh√¥ng qu√° s·ªë node\n        chromosome = np.zeros(n_nodes, dtype=int)\n        if n_CH > 0:\n            ch_indices = np.random.choice(n_nodes, n_CH, replace=False)\n            chromosome[ch_indices] = 1\n        return chromosome\n\n\n    def fitness_function(self, chromosome: np.ndarray, alive_nodes: List[Node]) -> float:\n        CH_local_indices = np.where(chromosome == 1)[0]\n        if len(CH_local_indices) == 0:\n            return 0.0\n        return self.evaluate_CH_fitness(alive_nodes, CH_local_indices.tolist())\n\n    def tournament_selection(self, population: List[np.ndarray],\n                            fitness_values: List[float]) -> np.ndarray:\n        tournament_indices = np.random.choice(len(population), self.params.tournament_size, replace=False)\n        # tournament_indices = np.random.choice(len(population), self.params.tournament_size, replace=True)\n        tournament_fitness = [fitness_values[i] for i in tournament_indices]\n        winner_idx = tournament_indices[np.argmax(tournament_fitness)]\n        return population[winner_idx].copy()\n\n    \n    def batch_fitness(self, population, alive_nodes):\n        fitness = []\n    \n        for chromo in population:\n            CH_local = np.where(chromo == 1)[0]\n            if len(CH_local) == 0:\n                fitness.append(0.0)\n                continue\n    \n            CH_global = [alive_nodes[i].idx for i in CH_local]\n            # ‚ö†Ô∏è ·ªû ƒë√¢y CH_indices l√† LOCAL index\n            f = self.evaluate_CH_fitness(alive_nodes, CH_local.tolist())\n            fitness.append(f)\n\n    \n        return fitness\n\n    \n    # def crossover(self, parent1: np.ndarray, parent2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    #     if np.random.rand() < self.params.crossover_rate:\n    #         point = np.random.randint(1, len(parent1))\n    #         child1 = np.concatenate([parent1[:point], parent2[point:]])\n    #         child2 = np.concatenate([parent2[:point], parent1[point:]])\n    #         return child1, child2\n    #     return parent1.copy(), parent2.copy()\n    def crossover(self, parent1: np.ndarray, parent2: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        # N·∫øu c√° th·ªÉ ch·ªâ c√≥ 1 gene ‚Üí b·ªè qua crossover\n        if len(parent1) <= 1 or len(parent2) <= 1:\n            return parent1.copy(), parent2.copy()\n    \n        if np.random.rand() < self.params.crossover_rate:\n            point = np.random.randint(1, len(parent1))\n            child1 = np.concatenate([parent1[:point], parent2[point:]])\n            child2 = np.concatenate([parent2[:point], parent1[point:]])\n        else:\n            child1, child2 = parent1.copy(), parent2.copy()\n    \n        return child1, child2\n\n\n    def mutate(self, chromosome: np.ndarray) -> np.ndarray:\n        for i in range(len(chromosome)):\n            if np.random.rand() < self.params.mutation_rate:\n                chromosome[i] = 1 - chromosome[i]\n        if chromosome.sum() == 0:\n            chromosome[np.random.randint(len(chromosome))] = 1\n        return chromosome\n\n    def select_cluster_heads(self, round_num: int) -> Tuple[List[int], float]:\n        alive_nodes = self.wsn.get_alive_nodes()\n        if not alive_nodes:\n            return [], 0.0\n\n        n_alive = len(alive_nodes)\n        # n_CH = max(1, int(self.net_params.p_CH * n_alive))\n        # n_CH = self.calculate_k(self.net_params, alive_nodes)\n        n_CH = min(self.calculate_k(self.net_params, alive_nodes), n_alive)\n\n        n_CH = min(n_CH, n_alive )\n        if n_CH == 0:\n            return np.array([]), 0  # Kh√¥ng c√≤n node ƒë·ªÉ ch·ªçn CH\n        ch_indices = np.random.choice(n_alive, n_CH, replace=False)\n\n\n\n        population = [self.create_chromosome(n_alive, n_CH)\n                     for _ in range(self.params.population_size)]\n\n        for generation in range(10):\n            fitness_values = [self.fitness_function(ch, alive_nodes) for ch in population]\n        \n            # 1. l·∫•y elite t·ª´ cha m·∫π\n            elite_indices = np.argsort(fitness_values)[-self.params.elite_size:]\n            elites = [population[i].copy() for i in elite_indices]\n        \n            # 2. t·∫°o th·∫ø h·ªá con\n            offspring = []\n            while len(offspring) < self.params.population_size:\n                p1 = self.tournament_selection(population, fitness_values)\n                p2 = self.tournament_selection(population, fitness_values)\n                c1, c2 = self.crossover(p1, p2)\n                offspring.append(self.mutate(c1))\n                offspring.append(self.mutate(c2))\n            offspring = offspring[:self.params.population_size]\n        \n            # 3. g·ªôp cha + con\n            combined = population + offspring\n        \n            # 4. t√≠nh fitness\n            combined_fitness = [self.fitness_function(ch, alive_nodes) for ch in combined]\n        \n            # 5. ch·ªçn top N c√° th·ªÉ t·ªët nh·∫•t\n            best_indices = np.argsort(combined_fitness)[-self.params.population_size:]\n            population = [combined[i].copy() for i in best_indices]\n        \n            # 6. thay th·∫ø elite v√†o ƒë·∫ßu (ƒë·∫£m b·∫£o elitism tuy·ªát ƒë·ªëi)\n            population[:self.params.elite_size] = elites\n\n        \n        \n\n\n        # fitness_values = [self.fitness_function(chromo, alive_nodes)\n        #                  for chromo in population]\n\n        fitness_values = self.batch_fitness(population, alive_nodes)\n\n        best_idx = np.argmax(fitness_values)\n        best_chromosome = population[best_idx]\n        best_fitness = fitness_values[best_idx]\n\n        CH_local_indices = np.where(best_chromosome == 1)[0]\n        # alive_nodes[i].idx lu√¥n l√† node real-id\n        CH_indices = [int(alive_nodes[i].idx) for i in CH_local_indices]\n        \n        # Ch·ªâ gi·ªØ CH c√≤n s·ªëng (energy > 0)\n        CH_indices = [cid for cid in CH_indices if self.wsn.nodes[cid].energy > 0]\n\n\n        return CH_indices, best_fitness\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.545104Z","iopub.execute_input":"2026-01-23T08:37:18.545472Z","iopub.status.idle":"2026-01-23T08:37:18.583870Z","shell.execute_reply.started":"2026-01-23T08:37:18.545433Z","shell.execute_reply":"2026-01-23T08:37:18.582441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport csv\nimport json\nimport numpy as np\nfrom datetime import datetime\nfrom typing import Dict, List, Tuple\nfrom time import time\nfrom time import time\n\n\nclass FileManager:\n    \"\"\"Manage input/output file operations with detailed round-by-round tracking\"\"\"\n    \n    def __init__(self, output_dir: str = \"simulation_results\"):\n        self.output_dir = output_dir\n        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        os.makedirs(output_dir, exist_ok=True)\n    \n    # ============================================================\n    # SAVE INPUT PARAMETERS + NODE POSITIONS\n    # ============================================================\n    def save_input_parameters(self, net_params, aco_params, ga_params, scenarios):\n        local_stamp = int(time() * 1000)\n        filename = f\"{self.output_dir}/input_parameters_{local_stamp}.txt\"\n    \n        with open(filename, mode='w') as f:\n            f.write(\"=== INPUT PARAMETERS ===\\n\\n\")\n    \n            for scenario_name, nodes in scenarios.items():\n                f.write(f\"Scenario: {scenario_name}\\n\")\n                print(f\"[DEBUG] Saving scenario: {scenario_name}\")\n    \n                # tr√°nh tr√πng file\n                local_stamp = int(time() * 1000)\n                pos_csv = f\"{self.output_dir}/initial_positions_{scenario_name}_{local_stamp}.csv\"\n    \n                # GHI TO·∫† ƒê·ªò NODE + BS\n                with open(pos_csv, mode='w', newline='') as csvfile:\n                    writer = csv.writer(csvfile)\n                    writer.writerow([\"NodeID\", \"X\", \"Y\"])\n    \n                    # ghi node\n                    for i, (x, y) in enumerate(nodes):\n                        writer.writerow([i, x, y])\n    \n                    # ghi tr·∫°m g·ªëc BS\n                    writer.writerow([\"BS\", net_params.BS_location[0], net_params.BS_location[1]])\n    \n                print(f\"  ‚úì Saved node positions (with BS): {pos_csv}\")\n                f.write(f\"    Positions saved: {pos_csv}\\n\")\n    \n            print(f\"‚úì Input parameters saved: {filename}\")\n\n    \n    # ============================================================\n    # SAVE DETAILED ROUND CLUSTERING (CSV)\n    # ============================================================\n    def save_round_clustering(self, scenario_name: str, algorithm_name: str, \n                              fitness_type: str, round_num: int,\n                              clusters: Dict[int, List[int]]):\n        \"\"\"\n        L∆∞u chi ti·∫øt ph√¢n c·ª•m t·ª´ng round: (round, CH_id, member_id)\n        Format: M·ªói d√≤ng = 1 th√†nh vi√™n trong cluster\n        \"\"\"\n        filename = f\"{self.output_dir}/round_clustering_{scenario_name}_{algorithm_name}_{fitness_type}_{self.timestamp}.csv\"\n        \n        file_exists = os.path.isfile(filename)\n        \n        with open(filename, 'a', newline='') as f:\n            writer = csv.writer(f)\n            \n            # Header ch·ªâ ghi l·∫ßn ƒë·∫ßu\n            if not file_exists:\n                writer.writerow([\"round\", \"cluster_head_id\", \"member_id\"])\n            \n            # Ghi t·ª´ng th√†nh vi√™n\n            for ch_id, members in clusters.items():\n                for member_id in members:\n                    writer.writerow([round_num, ch_id, member_id])\n        \n        if round_num == 1:  # Ch·ªâ log l·∫ßn ƒë·∫ßu ƒë·ªÉ tr√°nh spam\n            print(f\"  ‚úì Round clustering log started: {filename}\")\n    \n    # ============================================================\n    # SAVE ROUND STATISTICS (Energy, Fitness, etc.)\n    # ============================================================\n    def save_round_statistics(self, scenario_name: str, algorithm_name: str, \n                              fitness_type: str, round_num: int,\n                              stats: Dict, clusters: Dict[int, List[int]], \n                              nodes_info: List[Dict]):\n        \"\"\"Save comprehensive round statistics\"\"\"\n        \n        # 1Ô∏è‚É£ Summary metrics (alive nodes, energy, fitness)\n        summary_file = f\"{self.output_dir}/round_summary_{scenario_name}_{algorithm_name}_{fitness_type}_{self.timestamp}.csv\"\n        file_exists = os.path.isfile(summary_file)\n        \n        with open(summary_file, 'a', newline='') as f:\n            fieldnames = ['round', 'alive_nodes', 'dead_nodes', 'avg_energy', \n                          'total_energy', 'fitness', 'n_clusters']\n            writer = csv.DictWriter(f, fieldnames=fieldnames)\n            \n            if not file_exists:\n                writer.writeheader()\n            \n            writer.writerow({\n                'round': round_num,\n                'alive_nodes': stats['alive_nodes'][-1],\n                'dead_nodes': stats.get('dead_nodes', [0])[-1],\n                'avg_energy': stats['avg_energy'][-1],\n                'total_energy': stats['total_energy'][-1],\n                'fitness': stats['best_fitness'][-1],\n                'n_clusters': len(clusters)\n            })\n\n        # 2Ô∏è‚É£ Detailed node status\n        node_file = f\"{self.output_dir}/round_nodes_{scenario_name}_{algorithm_name}_{fitness_type}_{self.timestamp}.csv\"\n        node_file_exists = os.path.isfile(node_file)\n        \n        with open(node_file, 'a', newline='') as nf:\n            fieldnames = ['round', 'node_id', 'is_CH', 'cluster_id', 'energy', 'status']\n            writer = csv.DictWriter(nf, fieldnames=fieldnames)\n            \n            if not node_file_exists:\n                writer.writeheader()\n            \n            for node in nodes_info:\n                writer.writerow({\n                    'round': round_num,\n                    'node_id': node['id'],\n                    'is_CH': int(node.get('is_CH', 0)),\n                    'cluster_id': node.get('cluster_id', -1),\n                    'energy': node['energy'],\n                    'status': 'alive' if node['energy'] > 0 else 'dead'\n                })\n        \n        # 3Ô∏è‚É£ Clustering details (CH -> members)\n        self.save_round_clustering(scenario_name, algorithm_name, fitness_type, \n                                   round_num, clusters)\n\n    # ============================================================\n    # SAVE FINAL RESULTS\n    # ============================================================\n    def save_final_results(self, all_results: Dict[str, Dict[str, Dict]]):\n        \"\"\"Save summarized final results to TXT\"\"\"\n        filename = f\"{self.output_dir}/final_results_{self.timestamp}.txt\"\n        \n        with open(filename, 'w') as f:\n            f.write(f\"=== FINAL RESULTS ({self.timestamp}) ===\\n\\n\")\n            \n            for scenario_name, results in all_results.items():\n                f.write(f\"[Scenario: {scenario_name}]\\n\")\n                \n                for alg_name, fitness_dict in results.items():\n                    for fitness_type, stats in fitness_dict.items():\n                        fnd = min(stats['dead_rounds']) if stats['dead_rounds'] else None\n                        lnd = max(stats['dead_rounds']) if stats['dead_rounds'] else None\n                        \n                        f.write(f\"\\n  {alg_name} ({fitness_type}):\\n\")\n                        f.write(f\"    FND: {fnd} | LND: {lnd}\\n\")\n                        f.write(f\"    Total Energy: {stats['plot_cumulative_energy'][-1]:.4f} J\\n\")\n                        f.write(f\"    Avg Fitness: {np.mean(stats['best_fitness']):.4f}\\n\")\n                        f.write(f\"    Final Alive: {stats['alive_nodes'][-1]:.2f}%\\n\")\n                \n                f.write(\"-\" * 60 + \"\\n\\n\")\n        \n        print(f\"‚úì Final results saved: {filename}\")\n        return filename\n\n    # ============================================================\n    # SAVE FULL CLUSTER HISTORY (JSON)\n    # ============================================================\n    def save_cluster_history(self, scenario_name: str, algorithm_name: str, \n                            fitness_type: str, stats: Dict):\n        \"\"\"Save complete cluster history in JSON format\"\"\"\n        filename = f\"{self.output_dir}/cluster_history_{scenario_name}_{algorithm_name}_{fitness_type}_{self.timestamp}.json\"\n\n        history = []\n        for entry in stats.get(\"clusters_per_round\", []):\n            history.append({\n                \"round\": entry[\"round\"],\n                \"clusters\": {int(ch): members for ch, members in entry[\"clusters\"].items()}\n            })\n        \n        with open(filename, 'w') as f:\n            json.dump(history, f, indent=2)\n        \n        print(f\"‚úì Cluster history saved: {filename}\")\n        return filename\n\n    # ============================================================\n    # LOAD CLUSTER HISTORY\n    # ============================================================\n    def load_cluster_history(self, scenario_name: str, algorithm_name: str, \n                            fitness_type: str):\n        \"\"\"Load cluster history from JSON file\"\"\"\n        pattern = f\"cluster_history_{scenario_name}_{algorithm_name}_{fitness_type}_\"\n        \n        candidates = [\n            f for f in os.listdir(self.output_dir)\n            if f.startswith(pattern) and f.endswith(\".json\")\n        ]\n    \n        if not candidates:\n            print(f\"[WARNING] No cluster history found for: {pattern}\")\n            return None\n    \n        filename = max(\n            candidates,\n            key=lambda x: os.path.getmtime(os.path.join(self.output_dir, x))\n        )\n        full_path = os.path.join(self.output_dir, filename)\n    \n        with open(full_path, 'r') as f:\n            history = json.load(f)\n    \n        print(f\"‚úì Loaded cluster history: {full_path}\")\n        return {\"clusters_per_round\": history}\n\n    # ============================================================\n    # BEST ALGORITHM FINDER\n    # ============================================================\n    @staticmethod\n    def find_best_algorithm(all_results: Dict):\n        \"\"\"Find best algorithm per scenario based on avg fitness\"\"\"\n        best_summary = {}\n        \n        for scenario, algs in all_results.items():\n            best_avg_fit = -np.inf\n            best_algo = None\n            best_type = None\n            final_alive = 0.0\n\n            for alg_name, fitness_dict in algs.items():\n                for fitness_type, stats in fitness_dict.items():\n                    avg_fit = float(np.mean(stats['best_fitness']))\n                    alive_pct = float(stats['alive_nodes'][-1])\n\n                    if (avg_fit > best_avg_fit) or \\\n                       (avg_fit == best_avg_fit and alive_pct > final_alive):\n                        best_avg_fit = avg_fit\n                        best_algo = alg_name\n                        best_type = fitness_type\n                        final_alive = alive_pct\n\n            best_summary[scenario] = {\n                \"algorithm\": best_algo,\n                \"fitness_type\": best_type,\n                \"avg_fitness\": best_avg_fit,\n                \"final_alive_pct\": final_alive\n            }\n        \n        return best_summary\n        # ============================================================\n    # SAVE ROUND STATE (missing file you need)\n    # ============================================================\n    def save_round_state(self, scenario_name: str, algorithm_name: str,\n                         fitness_type: str, round_num: int,\n                         alive_nodes: int, dead_nodes: int,\n                         clusters: Dict[int, List[int]], best_fitness: float):\n        \"\"\"\n        L∆∞u tr·∫°ng th√°i round:\n        - round\n        - s·ªë node s·ªëng\n        - s·ªë node ch·∫øt\n        - s·ªë CH\n        - best fitness c·ªßa round\n        - danh s√°ch t·∫•t c·∫£ CH\n        \"\"\"\n\n        filename = f\"{self.output_dir}/round_state_{scenario_name}_{algorithm_name}_{fitness_type}_{self.timestamp}.csv\"\n        file_exists = os.path.isfile(filename)\n\n        with open(filename, 'a', newline='') as f:\n            fieldnames = [\n                \"round\",\n                \"alive_nodes\",\n                \"dead_nodes\",\n                \"n_clusters\",\n                \"best_fitness\",\n                \"cluster_heads\"\n            ]\n            writer = csv.DictWriter(f, fieldnames=fieldnames)\n\n            if not file_exists:\n                writer.writeheader()\n\n            CH_list = list(clusters.keys())\n\n            writer.writerow({\n                \"round\": round_num,\n                \"alive_nodes\": alive_nodes,\n                \"dead_nodes\": dead_nodes,\n                \"n_clusters\": len(CH_list),\n                \"best_fitness\": best_fitness,\n                \"cluster_heads\": json.dumps(CH_list)\n            })\n\n        if round_num == 1:\n            print(f\"  ‚úì Round state file started: {filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.585812Z","iopub.execute_input":"2026-01-23T08:37:18.586114Z","iopub.status.idle":"2026-01-23T08:37:18.631882Z","shell.execute_reply.started":"2026-01-23T08:37:18.586084Z","shell.execute_reply":"2026-01-23T08:37:18.630489Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom typing import List, Dict\n\ndef visualize_clusters_multihop_general(\n        wsn,\n        CH_indices: List[int],\n        clusters: Dict[int, List[int]],\n        net_params,\n        round_num: int,\n        scenario_name: str,\n        algorithm_name: str,\n        fitness_type: str,\n        output_dir: str):\n\n    \"\"\"\n    V·∫Ω bi·ªÉu ƒë·ªì clusters + multi-hop paths, l∆∞u ·∫£nh.\n    \"\"\"\n\n    # ===========================================================\n    # üî• FIX QUAN TR·ªåNG: convert m·ªçi key/members v·ªÅ int\n    # ===========================================================\n    clusters = {int(k): [int(m) for m in v] for k, v in clusters.items()}\n    CH_indices = [int(ch) for ch in CH_indices]\n\n    fig, ax = plt.subplots(figsize=(12, 12))\n    base_station = (net_params.area_size[0] / 2, net_params.area_size[1] / 2)\n\n    # M√†u s·∫Øc cho t·ª´ng CH\n    colors = plt.cm.tab20(np.linspace(0, 1, max(1, len(CH_indices))))\n    cluster_colors = {ch_idx: colors[i] for i, ch_idx in enumerate(CH_indices)}\n\n    # ===========================================================\n    # 1Ô∏è‚É£ V·∫Ω th√†nh vi√™n + ƒë∆∞·ªùng n·ªëi t·ªõi CH\n    # ===========================================================\n    for ch_idx, members in clusters.items():\n\n        if ch_idx not in CH_indices:\n            continue\n\n        ch_node = wsn.nodes[ch_idx]\n        color = cluster_colors[ch_idx]\n\n        for member_idx in members:\n            member_idx = int(member_idx)\n            member = wsn.nodes[member_idx]\n\n            ax.plot(\n                [member.x, ch_node.x], [member.y, ch_node.y],\n                color=color, linestyle='--', linewidth=0.8, alpha=0.4\n            )\n            ax.scatter(\n                member.x, member.y,\n                c=[color] if member.energy > 0 else ['gray'],\n                s=30,\n                marker='o' if member.energy > 0 else 'x',\n                alpha=0.6\n            )\n\n    # ===========================================================\n    # 2Ô∏è‚É£ V·∫Ω CH (h√¨nh vu√¥ng)\n    # ===========================================================\n    for ch_idx in CH_indices:\n        ch_node = wsn.nodes[ch_idx]\n        color = cluster_colors[ch_idx]\n\n        ax.scatter(\n            ch_node.x, ch_node.y,\n            s=200,\n            marker='s',\n            c=[color] if ch_node.energy > 0 else ['darkgray'],\n            edgecolors='black',\n            linewidth=2,\n            zorder=5\n        )\n\n    # ===========================================================\n    # 3Ô∏è‚É£ Multi-hop routing\n    # ===========================================================\n    from copy import deepcopy\n    CH_list = deepcopy(CH_indices)\n\n    # paths = {}\n\n    # # Distance CH -> BS\n    # dist_CH_to_BS = {\n    #     ch: np.sqrt(\n    #         (wsn.nodes[ch].x - base_station[0]) ** 2 +\n    #         (wsn.nodes[ch].y - base_station[1]) ** 2\n    #     )\n    #     for ch in CH_list\n    # }\n\n    # for ch in CH_list:\n\n    #     d_bs = dist_CH_to_BS[ch]\n\n    #     # CASE 1: g·∫ßn BS ‚Üí g·ª≠i tr·ª±c ti·∫øp\n    #     if d_bs <= net_params.d0:\n    #         paths[ch] = [ch]\n    #         continue\n\n    #     # CASE 2: ch·ªçn CH g·∫ßn BS nh·∫•t\n    #     possible_hops = {k: v for k, v in dist_CH_to_BS.items() if k != ch}\n\n    #     if not possible_hops:\n    #         paths[ch] = [ch]\n    #         continue\n\n    #     best_hop = min(possible_hops, key=possible_hops.get)\n    #     paths[ch] = [ch, best_hop]\n\n\n    R = net_params.R\n    paths = {}\n    \n    for ch in CH_indices:\n        path = [ch]\n        current = ch\n        visited = set([ch])\n    \n        while True:\n            curr_node = wsn.nodes[current]\n            d_bs = np.hypot(\n                curr_node.x - base_station[0],\n                curr_node.y - base_station[1]\n            )\n    \n            # ======================\n            # N·∫øu ƒë·ªß g·∫ßn BS ‚Üí k·∫øt th√∫c\n            # ======================\n            if d_bs < 2 * R:\n                path.append('BS')\n                break\n    \n            # ======================\n            # T√¨m CH trung gian\n            # ======================\n            candidates = []\n    \n            for other_ch in CH_indices:\n                if other_ch in visited:\n                    continue\n    \n                other = wsn.nodes[other_ch]\n    \n                d_ch_ch = np.hypot(\n                    curr_node.x - other.x,\n                    curr_node.y - other.y\n                )\n    \n                d_other_bs = np.hypot(\n                    other.x - base_station[0],\n                    other.y - base_station[1]\n                )\n    \n                if d_ch_ch <= R and d_other_bs < d_bs:\n                    candidates.append((other_ch, d_other_bs))\n    \n            # ======================\n            # Kh√¥ng c√≤n ƒë∆∞·ªùng ƒëi\n            # ======================\n            if not candidates:\n                path.append('BS')\n                break\n    \n            # ======================\n            # Ch·ªçn hop t·ªët nh·∫•t\n            # ======================\n            next_ch = min(candidates, key=lambda x: x[1])[0]\n    \n            path.append(next_ch)\n            visited.add(next_ch)\n            current = next_ch\n    \n        paths[ch] = path\n\n\n    # ===========================================================\n    # 3Ô∏è‚É£.5Ô∏è‚É£ V·∫Ω ƒë∆∞·ªùng multi-hop\n    # ===========================================================\n    for ch, path in paths.items():\n        for i in range(len(path) - 1):\n            src = path[i]\n            dst = path[i + 1]\n    \n            src_node = wsn.nodes[src]\n    \n            if dst == 'BS':\n                x2, y2 = base_station\n                style = ':'\n            else:\n                dst_node = wsn.nodes[dst]\n                x2, y2 = dst_node.x, dst_node.y\n                style = '-'\n    \n            ax.plot(\n                [src_node.x, x2],\n                [src_node.y, y2],\n                color='black',\n                linewidth=2,\n                linestyle=style\n            )\n\n\n    # ===========================================================\n    # 4Ô∏è‚É£ Base Station\n    # ===========================================================\n    ax.scatter(\n        base_station[0], base_station[1],\n        c='red', marker='*', s=800,\n        edgecolors='darkred', linewidth=3,\n        zorder=10\n    )\n\n    # ===========================================================\n    # 5Ô∏è‚É£ Dead nodes\n    # ===========================================================\n    dead_nodes = [n for n in wsn.nodes if n.energy <= 0]\n    if dead_nodes:\n        ax.scatter(\n            [n.x for n in dead_nodes],\n            [n.y for n in dead_nodes],\n            c='black', marker='x', s=50, alpha=0.5\n        )\n\n    # ===========================================================\n    # 6Ô∏è‚É£ Formatting\n    # ===========================================================\n    ax.set_xlim(-10, net_params.area_size[0] + 10)\n    ax.set_ylim(-10, net_params.area_size[1] + 10)\n    ax.set_xlabel(\"X (m)\")\n    ax.set_ylabel(\"Y (m)\")\n    ax.set_title(\n        f\"{algorithm_name} ({fitness_type}) - {scenario_name} | Round {round_num}\",\n        fontsize=14\n    )\n    ax.set_aspect('equal')\n    ax.grid(False, alpha=0.3)\n\n    # Info box\n    alive_nodes = len([n for n in wsn.nodes if n.energy > 0])\n    avg_energy = np.mean([n.energy for n in wsn.nodes if n.energy > 0]) if alive_nodes > 0 else 0\n    info_text = (\n        f\"Alive: {alive_nodes}/{net_params.n_nodes}\\n\"\n        f\"Avg Energy: {avg_energy:.4f}\\n\"\n        f\"Clusters: {len(CH_indices)}\"\n    )\n\n    ax.text(\n        0.02, 0.98, info_text,\n        transform=ax.transAxes,\n        verticalalignment='top',\n        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n    )\n    circle_2R = plt.Circle(base_station, 2 * R, color='orange',\n                       fill=False, linestyle='--', linewidth=2)\n    ax.add_patch(circle_2R)\n\n    # ===========================================================\n    # 7Ô∏è‚É£ Save\n    # ===========================================================\n    import os\n    os.makedirs(output_dir, exist_ok=True)\n\n    filename = (\n        f\"{output_dir}/clusters_multihop_{scenario_name}_\"\n        f\"{algorithm_name}_{fitness_type}_round{round_num}.png\"\n    )\n\n    plt.tight_layout()\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    plt.close()\n\n    print(f\"  ‚úì Cluster + multi-hop visualization saved: {filename}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.633635Z","iopub.execute_input":"2026-01-23T08:37:18.634008Z","iopub.status.idle":"2026-01-23T08:37:18.663648Z","shell.execute_reply.started":"2026-01-23T08:37:18.633970Z","shell.execute_reply":"2026-01-23T08:37:18.662635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_distributions(area_size: Tuple[int, int], n_nodes: int, \n                           scenarios: Dict[str, List[Tuple[float, float]]], \n                           output_dir: str):\n    \"\"\"Visualize node distributions (auto handle dynamic scenario count)\"\"\"\n\n    n_scenarios = len(scenarios)\n\n    # lu√¥n t·∫°o m·∫£ng 2D ƒë·ªÉ tr√°nh l·ªói indexing\n    fig, axes = plt.subplots(1, n_scenarios, figsize=(6*n_scenarios, 6), squeeze=False)\n    axes = axes[0]   # l·∫•y h√†ng ƒë·∫ßu ti√™n\n\n    for ax, (scenario_name, positions) in zip(axes, scenarios.items()):\n        xs, ys = zip(*positions)\n\n        ax.scatter(xs, ys, c='blue', alpha=0.6, s=50, edgecolors='darkblue', linewidth=0.5)\n        ax.scatter(area_size[0]/2, area_size[1]/2, c='red', marker='s',\n                   s=300, label='Base Station', edgecolors='darkred', linewidth=2)\n\n        ax.set_xlim(-10, area_size[0] + 10)\n        ax.set_ylim(-10, area_size[1] + 10)\n        ax.set_xlabel('X (m)', fontsize=12)\n        ax.set_ylabel('Y (m)', fontsize=12)\n        ax.set_title(scenario_name, fontweight='bold', fontsize=14)\n        ax.legend(fontsize=10)\n        ax.grid(True, alpha=0.3)\n        ax.set_aspect('equal')\n\n        ax.text(0.02, 0.98, f'Nodes: {n_nodes}',\n                transform=ax.transAxes, fontsize=10,\n                verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n    plt.tight_layout()\n    filename = f'{output_dir}/node_distributions.png'\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    plt.show()\n\n    print(f\"‚úì Distribution plot saved: {filename}\")\n\n\n\ndef plot_scenario_comparison(all_results: Dict[str, Dict[str, Dict]], \n                            net_params, output_dir: str):\n\n    scenarios = list(all_results.keys())\n    algorithms = ['GA', 'AC-ACO','Basic ACO']\n    fitness_types = ['old', 'new']\n    # fitness_types = [ 'new']\n    \n    colors = { 'GA': 'red','AC-ACO': 'blue', 'Basic ACO': 'green'}\n    linestyle_map = {'old': '-', 'new': '--'}\n    # linestyle_map = {'new': '-' }\n\n\n    # üî• S·ª¨A QUAN TR·ªåNG: LU√îN TR·∫¢ V·ªÄ axes 2D\n    fig, axes = plt.subplots(len(scenarios), 3, figsize=(18, 5*len(scenarios)), squeeze=False)\n    fig.suptitle('Multi-Scenario Algorithm Comparison (with 2 Fitness Types)', fontsize=18, fontweight='bold')\n\n    for row, scenario in enumerate(scenarios):\n        results = all_results[scenario]\n        \n        # Column 1: Network Lifetime\n        for alg in algorithms:\n            for fit_type in fitness_types:\n                print(scenario, alg,results[alg]['new']['alive_nodes'][0])\n                rounds = range(len(results[alg][fit_type]['alive_nodes']))\n                axes[row,0].plot(rounds, results[alg][fit_type]['alive_nodes'],\n                                  label=f'{alg} ({fit_type})', color=colors[alg],\n                                  linestyle=linestyle_map[fit_type], linewidth=2)\n        axes[row,0].set_title(f'{scenario} - Network Lifetime', fontweight='bold')\n        axes[row,0].set_xlabel('Rounds'); axes[row,0].set_ylabel('Alive Nodes (%)')\n        axes[row,0].legend(); axes[row,0].grid(True, alpha=0.3)\n\n        # Column 2: Cumulative Energy\n        for alg in algorithms:\n            for fit_type in fitness_types:\n                print(scenario, alg,         results[alg]['new']['alive_nodes'][0])\n                rounds = range(len(results[alg][fit_type]['plot_cumulative_energy']))\n                axes[row,1].plot(rounds, results[alg][fit_type]['plot_cumulative_energy'],\n                                  label=f'{alg} ({fit_type})', color=colors[alg],\n                                  linestyle=linestyle_map[fit_type], linewidth=2)\n        axes[row,1].set_title(f'{scenario} - Cumulative Energy', fontweight='bold')\n        axes[row,1].set_xlabel('Rounds'); axes[row,1].set_ylabel('Energy (J)')\n        axes[row,1].legend(); axes[row,1].grid(True, alpha=0.3)\n\n        # Column 3: Best Fitness\n        for alg in algorithms:\n            for fit_type in fitness_types:\n                print(scenario, alg,          results[alg]['new']['alive_nodes'][0])\n                rounds = range(len(results[alg][fit_type]['best_fitness']))\n                axes[row,2].plot(rounds, results[alg][fit_type]['best_fitness'],\n                                  label=f'{alg} ({fit_type})', color=colors[alg],\n                                  linestyle=linestyle_map[fit_type], linewidth=2, alpha=0.7)\n        axes[row,2].set_title(f'{scenario} - Best Fitness', fontweight='bold')\n        axes[row,2].set_xlabel('Rounds'); axes[row,2].set_ylabel('Fitness')\n        axes[row,2].legend(); axes[row,2].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    filename = f'{output_dir}/multi_scenario_comparison.png'\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    plt.show()\n    print(f\"‚úì Scenario comparison plot saved: {filename}\")\n\n\n\ndef plot_performance_summary(all_results: Dict[str, Dict[str, Dict]], output_dir: str):\n    \"\"\"Create summary bar charts comparing algorithms across fitness types\"\"\"\n    scenarios = list(all_results.keys())\n    algorithms = ['GA','AC-ACO', 'Basic ACO']\n    fitness_types = ['old', 'new']\n    # fitness_types = [ 'new']\n\n    metrics = ['FND', 'LND', 'Total Energy', 'Avg Fitness']\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    fig.suptitle('Performance Summary Across Scenarios (with 2 Fitness Types)', fontsize=16, fontweight='bold')\n\n    x = np.arange(len(scenarios))\n    width = 0.15\n\n    for idx, metric_name in enumerate(metrics):\n        ax = axes[idx//2, idx%2]\n        for i, alg in enumerate(algorithms):\n            for j, fit_type in enumerate(fitness_types):\n                values = []\n                for scenario in scenarios:\n                    stats = all_results[scenario][alg][fit_type]\n                    if metric_name == 'FND':\n                        values.append(min(stats['dead_rounds']) if stats['dead_rounds'] else 3000)\n                    elif metric_name == 'LND':\n                        values.append(max(stats['dead_rounds']) if stats['dead_rounds'] else 3000)\n                    elif metric_name == 'Total Energy':\n                        values.append(stats['plot_cumulative_energy'][-1])\n                    elif metric_name == 'Avg Fitness':\n                        values.append(np.mean(stats['best_fitness']))\n                offset = width * (i*2 + j - 3)\n                ax.bar(x + offset, values, width, label=f'{alg} ({fit_type})', alpha=0.8)\n\n        ax.set_xlabel('Scenario', fontsize=11)\n        ax.set_ylabel(metric_name, fontsize=11)\n        ax.set_title(f'{metric_name} Comparison', fontweight='bold')\n        ax.set_xticks(x)\n        ax.set_xticklabels(scenarios, rotation=15, ha='right')\n        ax.legend()\n        ax.grid(True, alpha=0.3, axis='y')\n\n    plt.tight_layout()\n    filename = f'{output_dir}/performance_summary.png'\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    plt.show()\n    print(f\"‚úì Performance summary plot saved: {filename}\")\n\n\ndef plot_additional_metrics(all_results: Dict[str, Dict[str, Dict]], output_dir: str):\n\n    scenarios = list(all_results.keys())\n    algorithms = [ 'GA', 'AC-ACO', 'Basic ACO']\n    fitness_types = ['old', 'new']\n    # fitness_types = [ 'new']\n    \n    colors = { 'GA': 'red','AC-ACO': 'blue', 'Basic ACO': 'green'}\n    linestyle_map = {'old': '-' , 'new': '--'}\n    # linestyle_map = {'new': '-' }\n\n\n    # üî• S·ª¨A QUAN TR·ªåNG: LU√îN TR·∫¢ V·ªÄ axes 2D\n    fig, axes = plt.subplots(len(scenarios), 2, figsize=(16, 5*len(scenarios)), squeeze=False)\n    fig.suptitle('Additional Metrics & Best Fitness Algorithm Highlight', fontsize=18, fontweight='bold')\n\n    for row, scenario in enumerate(scenarios):\n        results = all_results[scenario]\n\n        # Average Energy\n        for alg in algorithms:\n            for fit_type in fitness_types:\n                rounds = range(len(results[alg][fit_type]['avg_energy']))\n                axes[row,0].plot(rounds, results[alg][fit_type]['avg_energy'],\n                                 label=f'{alg} ({fit_type})', color=colors[alg],\n                                 linestyle=linestyle_map[fit_type], linewidth=2)\n        axes[row,0].set_title(f'{scenario} - Average Node Energy', fontweight='bold')\n        axes[row,0].set_xlabel('Rounds'); axes[row,0].set_ylabel('Average Energy (J)')\n        axes[row,0].legend(); axes[row,0].grid(True, alpha=0.3)\n\n        # Number of clusters\n        for alg in algorithms:\n            for fit_type in fitness_types:\n                rounds = range(len(results[alg][fit_type]['n_clusters']))\n                axes[row,1].plot(rounds, results[alg][fit_type]['n_clusters'],\n                                 label=f'{alg} ({fit_type})', color=colors[alg],\n                                 linestyle=linestyle_map[fit_type], linewidth=2)\n        axes[row,1].set_title(f'{scenario} - Number of Clusters', fontweight='bold')\n        axes[row,1].set_xlabel('Rounds'); axes[row,1].set_ylabel('Number of CHs')\n        axes[row,1].legend(); axes[row,1].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    filename = f'{output_dir}/additional_metrics.png'\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    plt.show()\n    print(f\"‚úì Additional metrics plot saved: {filename}\")\n\n\n\n\ndef plot_cluster_count_comparison(all_results: Dict[str, Dict[str, Dict]],\n                                  output_dir: str):\n\n    scenarios = list(all_results.keys())\n    algorithms = ['GA', 'AC-ACO', 'Basic ACO']\n    fitness_types = ['old', 'new']\n    # fitness_types = ['new' ]\n    \n    colors = {'GA': 'red', 'AC-ACO': 'blue', 'Basic ACO': 'green'}\n    linestyle_map = {'old': '-', 'new': '--'}\n    # linestyle_map = {'new': '-'}\n    fig, axes = plt.subplots(\n        len(scenarios), 1,\n        figsize=(12, 5 * len(scenarios)),\n        squeeze=False\n    )\n\n    fig.suptitle(\n        'Cluster Count Comparison per Round',\n        fontsize=18, fontweight='bold'\n    )\n\n    for row, scenario in enumerate(scenarios):\n        results = all_results[scenario]\n        ax = axes[row, 0]\n\n        for alg in algorithms:\n            for fit_type in fitness_types:\n\n                clusters_per_round = results[alg][fit_type].get(\n                    'clusters_per_round', []\n                )\n\n                if not clusters_per_round:\n                    continue\n\n                # üîë CHUY·ªÇN clusters_per_round ‚Üí n_clusters\n                rounds = []\n                n_clusters = []\n\n                for r in clusters_per_round:\n                    rounds.append(r['round'])\n                    n_clusters.append(len(r['clusters']))\n\n                ax.plot(\n                    rounds,\n                    n_clusters,\n                    label=f'{alg} ({fit_type})',\n                    color=colors[alg],\n                    linestyle=linestyle_map[fit_type],\n                    linewidth=2\n                )\n\n        ax.set_title(f'{scenario} - Number of Clusters per Round', fontweight='bold')\n        ax.set_xlabel('Round')\n        ax.set_ylabel('Number of Clusters (CHs)')\n        ax.legend()\n        ax.grid(False)\n\n    plt.tight_layout()\n    filename = f'{output_dir}/cluster_count_comparison.png'\n    plt.savefig(filename, dpi=300, bbox_inches='tight')\n    plt.show()\n\n    print(f\"‚úì Cluster count comparison plot saved: {filename}\")\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.665110Z","iopub.execute_input":"2026-01-23T08:37:18.665597Z","iopub.status.idle":"2026-01-23T08:37:18.710962Z","shell.execute_reply.started":"2026-01-23T08:37:18.665565Z","shell.execute_reply":"2026-01-23T08:37:18.709722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# SUMMARY TABLE PRINTING\n# ============================================================================\n\ndef print_scenario_summary(all_results: Dict[str, Dict[str, Dict]]):\n    \"\"\"\n    Print detailed comparison table including two fitness types.\n    Highlights best algorithm per scenario for each metric.\n    \"\"\"\n    print(\"\\n\" + \"=\"*120)\n    print(\"MULTI-SCENARIO PERFORMANCE SUMMARY (300 NODES, 1 FITNESS TYPES)\")\n    print(\"=\"*120)\n    \n    scenarios = list(all_results.keys())\n    algorithms = ['GA', 'AC-ACO','Basic ACO']\n    fitness_types = ['old' , 'new']\n    # fitness_types = ['new' ]\n    # Header\n    header = f\"{'Metric':<20} {'Scenario':<15}\"\n    for alg in algorithms:\n        for fit in fitness_types:\n            header += f\" {alg}({fit})\".ljust(15)\n    print(\"\\n\" + header)\n    print(\"-\" * 120)\n    \n    # Metrics to print\n    metrics = ['FND', 'LND', 'Total Energy (J)', 'Avg Fitness', 'Final Alive %']\n    \n    for metric in metrics:\n        for scenario in scenarios:\n            row = f\"{metric:<20} {scenario:<15}\"\n            \n            for alg in algorithms:\n                for fit in fitness_types:\n                    stats = all_results[scenario][alg][fit]\n                    \n                    if metric == 'FND':\n                        value = min(stats['dead_rounds']) if stats['dead_rounds'] else 3000\n                        row += f\"{value:<15d}\"\n                    elif metric == 'LND':\n                        value = max(stats['dead_rounds']) if stats['dead_rounds'] else 3000\n                        row += f\"{value:<15d}\"\n                    elif metric == 'Total Energy (J)':\n                        value = stats['plot_cumulative_energy'][-1]\n                        row += f\"{value:<15.2f}\"\n                    elif metric == 'Avg Fitness':\n                        value = np.mean(stats['best_fitness'])\n                        row += f\"{value:<15.4f}\"\n                    elif metric == 'Final Alive %':\n                        value = stats['alive_nodes'][-1]\n                        row += f\"{value:<15.1f}\"\n            \n            print(row)\n        print(\"-\" * 120)\n    \n    # Best algorithm/fitness per scenario\n    print(\"\\n\" + \"=\"*120)\n    print(\"BEST ALGORITHM + FITNESS PER SCENARIO\")\n    print(\"=\"*120)\n    \n    for scenario in scenarios:\n        print(f\"\\n{scenario}:\")\n        \n        # Best FND (largest round of first node death)\n        fnd_values = {}\n        for alg in algorithms:\n            for fit in fitness_types:\n                stats = all_results[scenario][alg][fit]\n                fnd_values[f\"{alg}({fit})\"] = min(stats['dead_rounds']) if stats['dead_rounds'] else 3000\n        best_fnd = max(fnd_values, key=fnd_values.get)\n        print(f\"  ‚Ä¢ Best FND (Stability): {best_fnd} - Round {fnd_values[best_fnd]}\")\n        \n        # Best Energy Efficiency (lowest total energy consumed)\n        energy_values = {}\n        for alg in algorithms:\n            for fit in fitness_types:\n                stats = all_results[scenario][alg][fit]\n                energy_values[f\"{alg}({fit})\"] = stats['plot_cumulative_energy'][-1]\n        best_energy = min(energy_values, key=energy_values.get)\n        print(f\"  ‚Ä¢ Best Energy Efficiency: {best_energy} - {energy_values[best_energy]:.2f} J\")\n        \n        # Best Average Fitness\n        fitness_values = {}\n        for alg in algorithms:\n            for fit in fitness_types:\n                stats = all_results[scenario][alg][fit]\n                fitness_values[f\"{alg}({fit})\"] = np.mean(stats['best_fitness'])\n        best_fitness = max(fitness_values, key=fitness_values.get)\n        print(f\"  ‚Ä¢ Best Avg Fitness: {best_fitness} - {fitness_values[best_fitness]:.4f}\")\n        \n        # Best Final Alive %\n        alive_values = {}\n        for alg in algorithms:\n            for fit in fitness_types:\n                stats = all_results[scenario][alg][fit]\n                alive_values[f\"{alg}({fit})\"] = stats['alive_nodes'][-1]\n        best_alive = max(alive_values, key=alive_values.get)\n        print(f\"  ‚Ä¢ Best Final Alive %: {best_alive} - {alive_values[best_alive]:.1f}\")\n    \n    print(\"\\n\" + \"=\"*120)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.712538Z","iopub.execute_input":"2026-01-23T08:37:18.712816Z","iopub.status.idle":"2026-01-23T08:37:18.750761Z","shell.execute_reply.started":"2026-01-23T08:37:18.712789Z","shell.execute_reply":"2026-01-23T08:37:18.749461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# MULTI-RUN ANALYSIS MODULE\n# Fixed node positions + multiple runs statistical analysis\n# ============================================================================\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom collections import defaultdict\nimport os\n\n\n# =========================\n# 1. MULTI-RUN EXECUTION\n# =========================\ndef run_multiple_times(\n    n_runs,\n    net_params,\n    aco_params,\n    ga_params,\n    scenario_name,\n    node_positions,\n    file_manager,\n    run_func\n):\n    \"\"\"\n    Run the same scenario multiple times with fixed node positions.\n    \"\"\"\n    all_runs_results = []\n\n    for run_id in range(n_runs):\n        print(f\"\\nüöÄ MULTI-RUN {run_id + 1}/{n_runs}\")\n\n        results = run_func(\n            net_params,\n            aco_params,\n            ga_params,\n            scenario_name,\n            node_positions,\n            file_manager\n        )\n\n        all_runs_results.append(results)\n\n    return all_runs_results\n\n\n# =========================\n# 2. METRIC EXTRACTION\n# =========================\ndef extract_metric(all_runs_results, algorithm, fitness_type, metric_name):\n    \"\"\"\n    Extract a metric across multiple runs.\n    \"\"\"\n    values = []\n\n    for run in all_runs_results:\n        try:\n            value = run[algorithm][fitness_type][metric_name]\n            values.append(value)\n        except KeyError:\n            print(\n                f\"[WARN] Missing {metric_name} for \"\n                f\"{algorithm} ({fitness_type})\"\n            )\n\n    return values\n\n\ndef collect_all_metrics(all_runs_results, algorithms, fitness_types, metric_name):\n    \"\"\"\n    Collect metrics for all algorithm + fitness combinations.\n    \"\"\"\n    metric_data = {}\n\n    for alg in algorithms:\n        for fit in fitness_types:\n            key = f\"{alg}_{fit}\"\n            metric_data[key] = extract_metric(\n                all_runs_results, alg, fit, metric_name\n            )\n\n    return metric_data\n\n\n# =========================\n# 3. VISUALIZATION\n# =========================\ndef plot_boxplot(metric_data, ylabel, title, output_path):\n    plt.figure(figsize=(10, 5))\n    plt.boxplot(metric_data.values(), labels=metric_data.keys())\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.xticks(rotation=20)\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\n\ndef plot_histogram_with_gaussian(metric_data, title, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n\n    for label, data in metric_data.items():\n        n = len(data)\n\n        if n < 15:\n            print(\n                f\"[SKIP] Gaussian fit skipped for {label} \"\n                f\"(n={n} < 15)\"\n            )\n            continue\n\n        mu, std = norm.fit(data)\n        bins = int(np.sqrt(n))\n\n        plt.figure(figsize=(6, 4))\n        plt.hist(data, bins=bins, density=True, alpha=0.6)\n        x = np.linspace(min(data), max(data), 200)\n        plt.plot(x, norm.pdf(x, mu, std), linewidth=2)\n        plt.title(f\"{label}\\nŒº={mu:.2f}, œÉ={std:.2f}\")\n        plt.tight_layout()\n        plt.savefig(os.path.join(output_dir, f\"hist_{label}.png\"))\n        plt.close()\n\n\n\ndef plot_mean_std_bar(metric_data, ylabel, title, output_path):\n    labels = list(metric_data.keys())\n    means = [np.mean(v) for v in metric_data.values()]\n    stds = [np.std(v) for v in metric_data.values()]\n\n    plt.figure(figsize=(10, 5))\n    plt.bar(labels, means, yerr=stds, capsize=6)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.xticks(rotation=20)\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\n\n# =========================\n# 4. MASTER ANALYSIS PIPELINE\n# =========================\ndef analyze_multi_run_results(\n    all_runs_results,\n    metric_name,\n    ylabel,\n    scenario_name,\n    output_dir\n):\n    \"\"\"\n    Full statistical analysis pipeline.\n    \"\"\"\n    algorithms = list(all_runs_results[0].keys())\n    fitness_types = list(\n        all_runs_results[0][algorithms[0]].keys()\n    )\n\n    metric_data = collect_all_metrics(\n        all_runs_results,\n        algorithms,\n        fitness_types,\n        metric_name\n    )\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Boxplot\n    plot_boxplot(\n        metric_data,\n        ylabel=ylabel,\n        title=f\"{metric_name} Distribution ({scenario_name}, fixed nodes)\",\n        output_path=os.path.join(\n            output_dir, f\"boxplot_{metric_name}.png\"\n        )\n    )\n\n    # Histogram + Gaussian\n    plot_histogram_with_gaussian(\n        metric_data,\n        title=f\"{metric_name} Distribution\",\n        output_dir=os.path.join(output_dir, \"histograms\")\n    )\n\n    # Mean ¬± Std\n    plot_mean_std_bar(\n        metric_data,\n        ylabel=ylabel,\n        title=f\"{metric_name} Mean ¬± Std ({scenario_name})\",\n        output_path=os.path.join(\n            output_dir, f\"mean_std_{metric_name}.png\"\n        )\n    )\n\n    print(f\"\\n‚úÖ Multi-run analysis completed for metric: {metric_name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:18.752099Z","iopub.execute_input":"2026-01-23T08:37:18.752529Z","iopub.status.idle":"2026-01-23T08:37:19.487250Z","shell.execute_reply.started":"2026-01-23T08:37:18.752477Z","shell.execute_reply":"2026-01-23T08:37:19.486441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom typing import List, Tuple\nimport os\n\ndef load_topology_with_bs(csv_path: str) -> Tuple[List[Tuple[float, float]], Tuple[float, float]]:\n    df = pd.read_csv(csv_path)\n\n    # T√°ch BS\n    bs_row = df[df['NodeID'] == 'BS']\n    assert len(bs_row) == 1, \"File must contain exactly one BS row\"\n\n    bs_position = (\n        float(bs_row.iloc[0]['X']),\n        float(bs_row.iloc[0]['Y'])\n    )\n\n    # L·∫•y node th∆∞·ªùng\n    node_df = df[df['NodeID'] != 'BS']\n\n    node_positions = list(\n        zip(node_df['X'].astype(float),\n            node_df['Y'].astype(float))\n    )\n\n    return node_positions, bs_position\n\n\n\n\n\ndef get_single_input_file(input_dir=\"/kaggle/input\"):\n    for root, dirs, files in os.walk(input_dir):\n        for f in files:\n            if f.lower().endswith(\".csv\"):\n                return os.path.join(root, f)\n    raise FileNotFoundError(\"No CSV file found in Kaggle input\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:19.488327Z","iopub.execute_input":"2026-01-23T08:37:19.488822Z","iopub.status.idle":"2026-01-23T08:37:19.496857Z","shell.execute_reply.started":"2026-01-23T08:37:19.488792Z","shell.execute_reply":"2026-01-23T08:37:19.495742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# MAIN SIMULATION RUNNER - UPDATED FOR 2 FITNESS TYPES & MULTI-HOP VISUALIZATION\n# ============================================================================\n\ndef run_scenario_comparison(net_params: NetworkParams, aco_params: ACOParams,\n                           ga_params: GAParams, scenario_name: str,\n                           node_positions: List[Tuple[float, float]],\n                           file_manager: FileManager):\n    \"\"\"\n    Run all algorithms on a specific scenario for both fitness types.\n    Returns structured results.\n    \"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"SCENARIO: {scenario_name.upper()}\")\n    print(f\"{'='*80}\")\n    \n    base_wsn = WSN(net_params, node_positions)\n    results = {}\n    \n    algorithms = {\n   \n        \"AC-ACO\": ACACO,\n        \"Basic ACO\": BasicACO,\n        \"GA\": GeneticAlgorithm\n\n        \n\n        \n    }\n    \n    fitness_types = ['old', 'new']\n    # fitness_types = [ 'new']\n    \n    \n    \n    for alg_name, AlgClass in algorithms.items():\n        results[alg_name] = {}\n        for fit_type in fitness_types:\n            print(f\"\\n[{scenario_name}] Running {alg_name} with fitness: {fit_type}...\")\n            wsn_copy = base_wsn.copy()\n            # --- inside run_scenario_comparison ---\n            if alg_name == \"GA\":\n                alg_instance = AlgClass(\n                    wsn_copy,\n                    ga_params,\n                    net_params,\n                    file_manager=file_manager,\n                    scenario_name=scenario_name,\n                    fitness_type=fit_type\n                )\n            else:\n                alg_instance = AlgClass(\n                    wsn_copy,\n                    aco_params,\n                    file_manager=file_manager,\n                    scenario_name=scenario_name,\n                    fitness_type=fit_type\n                )\n            \n            # useful for internal use in run_round()\n            alg_instance.algorithm_name = alg_name\n\n            \n            stats = alg_instance.run_simulation(alg_name, log_interval=250)\n            results[alg_name][fit_type] = stats\n            \n            # Save cluster history\n            file_manager.save_cluster_history(\n                scenario_name=scenario_name,\n                algorithm_name=alg_name,\n                stats=stats,\n                fitness_type=fit_type\n            )\n\n    \n    print(f\"\\n‚úì [{scenario_name}] All algorithms (2 fitness types) completed!\")\n    return results\n\n# from multi_run_analysis import (\n#     run_multiple_times,\n#     analyze_multi_run_results\n# )\n\ndef main():\n\n    CHECKPOINT_DIR = \"/kaggle/working/wsn_checkpoints\"\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n    \n    \"\"\"Main execution function for multi-fitness WSN simulation\"\"\"\n    print(\"=\"*120)\n    print(\"WSN CLUSTERING SIMULATION: 300 NODES, SINGLE-FITNESS COMPARISON\")\n    print(\"Scenarios: Uniform Random\")\n    print(\"Algorithms: AC-ACO, Basic ACO, Genetic Algorithm\")\n    print(\"Fitness Types: old, new\")\n    print(\"=\"*120)\n    \n    # Initialize parameters\n    net_params = NetworkParams(n_nodes=200)\n    aco_params = ACOParams()\n    ga_params = GAParams()\n    file_manager = FileManager()\n    \n    # Generate node positions\n    print(\"\\nGenerating node distributions...\")\n    # scenarios = {\n    #     'Uniform Random': NodeDistribution.uniform_random(\n    #         net_params.n_nodes, net_params.area_size)\n    # }\n\n\n    # =========================\n    # LOAD TOPOLOGY FROM FILE\n    # =========================\n\n    csv_path = get_single_input_file(\"/kaggle/input/200-node\")\n\n    node_positions, bs_position = load_topology_with_bs(csv_path)\n\n    # node_positions, bs_position = load_topology_with_bs(\n    #     \"data/node_positions.csv\"\n    # )\n    \n    net_params.n_nodes = len(node_positions)\n    net_params.bs_position = bs_position\n\n\n    \n    # fixed_positions = NodeDistribution.uniform_random(\n    #     net_params.n_nodes,\n    #     net_params.area_size,\n    #     # n_clusters=4\n    # )\n    \n    # scenarios = {\n    #     # # 1Ô∏è‚É£ Ph√¢n b·ªë ng·∫´u nhi√™n ƒë·ªÅu (baseline ‚Äì b·∫Øt bu·ªôc c√≥ trong paper)\n    #     'Uniform Random': NodeDistribution.uniform_random(\n    #         net_params.n_nodes, net_params.area_size\n    #     ),\n    \n    #     # # 2Ô∏è‚É£ D·ªìn v·ªÅ g√≥c (stress test cho multi-hop & CH selection)\n    #     # 'Corner Biased': NodeDistribution.corner_biased(\n    #     #     net_params.n_nodes, net_params.area_size, bias_ratio=0.85\n    #     # ),\n    \n    #     # 3Ô∏è‚É£ Ph√¢n c·ª•m r√µ r·ªát (r·∫•t hay d√πng cho ACO / clustering paper)\n    #     # 'Clustered': NodeDistribution.clustered(\n    #     #     net_params.n_nodes, net_params.area_size, n_clusters=4\n    #     # ),\n    # }\n\n\n    # =========================\n    # FIXED SCENARIO FROM FILE\n    # =========================\n    scenarios = {\n        'Uniform Random': node_positions\n    }\n    \n    fixed_positions = node_positions\n    \n\n    # Save input parameters\n    file_manager.save_input_parameters(net_params, aco_params, ga_params, scenarios)\n    \n    # Visualize initial node distributions\n    visualize_distributions(net_params.area_size, net_params.n_nodes, \n                            scenarios, file_manager.output_dir)\n    \n    # Run simulations for all scenarios\n    # all_results = {}\n    # for scenario_name, positions in scenarios.items():\n    #     results = run_scenario_comparison(\n    #         net_params, aco_params, ga_params,\n    #         scenario_name, positions, file_manager\n    #     )\n    #     all_results[scenario_name] = results\n     # =========================\n    # 3Ô∏è‚É£ MULTI-RUN EXECUTION\n    # =========================\n    N_RUNS = 1\n    SCENARIO = \"Uniform Random\"\n    \n    all_runs_results = run_multiple_times(\n        n_runs=N_RUNS,\n        net_params=net_params,\n        aco_params=aco_params,\n        ga_params=ga_params,\n        scenario_name=SCENARIO,\n        node_positions=fixed_positions,\n        file_manager=file_manager,\n        run_func=run_scenario_comparison\n    )\n\n    # =========================\n    # 4Ô∏è‚É£ STATISTICAL ANALYSIS\n    # =========================\n    analyze_multi_run_results(\n        all_runs_results,\n        metric_name=\"network_lifetime\",\n        ylabel=\"Network Lifetime (rounds)\",\n        scenario_name=\"Clustered\",\n        output_dir=file_manager.output_dir + \"/multi_run_analysis\"\n    )\n\n    all_results = {\n        \"Uniform Random\": all_runs_results[0]\n    }\n    print(\"üîÅ Re-running ONE simulation for visualization\")\n    \n    single_results = run_scenario_comparison(\n        net_params,\n        aco_params,\n        ga_params,\n        scenario_name=SCENARIO,\n        node_positions=fixed_positions,\n        file_manager=file_manager\n    )\n\n    \n\n    \n    print(\"\\n‚úÖ MULTI-RUN ANALYSIS COMPLETE\")\n    # Save final results\n    file_manager.save_final_results(all_results)\n    \n    # Print summary table (multi-fitness)\n    print_scenario_summary(all_results)\n    \n    # Generate all plots\n    print(\"\\nGenerating visualization plots...\")\n    plot_scenario_comparison(all_results, net_params, file_manager.output_dir)\n    plot_performance_summary(all_results, file_manager.output_dir)\n    plot_additional_metrics(all_results, file_manager.output_dir)\n    \n    plot_cluster_count_comparison(\n        all_results,\n        file_manager.output_dir\n    )\n\n\n    \n    # Multi-hop visualization for round 250\n    round_num = 1000\n   \n    for scenario_name in scenarios:\n        for alg_name in ['GA','AC-ACO', 'Basic ACO']:\n            for fit_type in ['old','new'] :\n                             # , 'new']:\n                # Load cluster history for round 250\n                # stats = file_manager.load_cluster_history(scenario_name, alg_name, fitness_type=fit_type)\n                stats = file_manager.load_cluster_history(\n                    scenario_name, alg_name, fitness_type=fit_type\n                )\n                \n                if stats is None:\n                    print(\n                        f\"‚ùå NO cluster history found | \"\n                        f\"{scenario_name} | {alg_name} | {fit_type}\"\n                    )\n                    continue\n\n                # round_data = next((r for r in stats['clusters_per_round'] if r['round']==round_num), None)\n                round_data = next(\n                    (r for r in stats['clusters_per_round'] if r['round'] == round_num),\n                    None\n                )\n                \n                if round_data is None:\n                    max_round = (\n                        max(r['round'] for r in stats['clusters_per_round'])\n                        if stats['clusters_per_round'] else None\n                    )\n                \n                    print(\n                        f\"‚ùå {scenario_name} | {alg_name} | {fit_type} \"\n                        f\"‚Üí NO data at round {round_num} \"\n                        f\"(max saved round = {max_round})\"\n                    )\n                    continue\n                else:\n                    print(\n                        f\"‚úÖ {scenario_name} | {alg_name} | {fit_type} \"\n                        f\"‚Üí FOUND round {round_num} \"\n                        f\"(clusters = {len(round_data['clusters'])})\"\n                    )\n                \n                    clusters = round_data['clusters']\n                    CH_indices = list(clusters.keys())\n\n                if round_data:\n                    clusters = round_data['clusters']\n                    CH_indices = list(clusters.keys())\n                    visualize_clusters_multihop_general(\n                        # wsn=WSN(net_params, scenarios[scenario_name]),\n                        wsn=WSN(net_params, fixed_positions),\n                        CH_indices=CH_indices,\n                        clusters=clusters,\n                        net_params=net_params,\n                        round_num=round_num,\n                        scenario_name=scenario_name,\n                        algorithm_name=alg_name,\n                        fitness_type=fit_type,\n                        output_dir=file_manager.output_dir\n                    )\n\n\n    print(\"\\n\" + \"=\"*120)\n    print(\"‚úì SIMULATION COMPLETE!\")\n    print(f\"‚úì Results saved in: {file_manager.output_dir}/\")\n    print(\"‚úì Generated files include:\")\n    print(\"  - input_parameters_*.json (input configuration)\")\n    print(\"  - final_results_*.json (output results)\")\n    print(\"  - node_distributions_300.png\")\n    print(\"  - multi_scenario_comparison.png\")\n    print(\"  - performance_summary.png\")\n    print(\"  - additional_metrics.png\")\n    print(\"  - clusters_multihop_*.png (round 250)\")\n    print(\"=\"*120)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:19.498292Z","iopub.execute_input":"2026-01-23T08:37:19.498945Z","iopub.status.idle":"2026-01-23T08:37:19.527650Z","shell.execute_reply.started":"2026-01-23T08:37:19.498912Z","shell.execute_reply":"2026-01-23T08:37:19.526562Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-23T08:37:19.528755Z","iopub.execute_input":"2026-01-23T08:37:19.529068Z","iopub.status.idle":"2026-01-23T09:08:58.194962Z","shell.execute_reply.started":"2026-01-23T08:37:19.529041Z","shell.execute_reply":"2026-01-23T09:08:58.193252Z"}},"outputs":[],"execution_count":null}]}